{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xODusJLlZUf"
      },
      "source": [
        "This is an abbreviated version of the c1_onboarding.ipynb notebook that specifically focuses on artifacts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i0N-GAivDsw"
      },
      "source": [
        "## ü™Ñ Install `wandb` library and login\n",
        "\n",
        "\n",
        "The first step on our journey is to install the client, which is as easy as:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6yGlDervEh8",
        "outputId": "5cc282f8-b5c3-43c9-b1de-3090c001f61c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install c1_aiml_aem -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojUNT0WhGLjc",
        "outputId": "48c52488-eb86-45f8-fa46-9000f07d3905"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy<1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"numpy<1.26.4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ6505_yammL",
        "outputId": "4294be3b-c06a-4282-ae4e-1d799b425692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.26.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E83xgBm8yjn"
      },
      "source": [
        "## Log in to W&B\n",
        "- You can explicitly login using `wandb login` or `wandb.login()` (See below)\n",
        "- Alternatively you can set environment variables. There are several env variables which you can set to change the behavior of W&B logging. The most important are:\n",
        "    - `WANDB_API_KEY` - find this in your \"Settings\" section under your profile\n",
        "    - `WANDB_BASE_URL` - this is the url of the W&B server\n",
        "- Find your API Token in \"Profile\" -> \"Setttings\" in the W&B App\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2oj15Y_Eyt5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from c1_aiml_aem import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsNzSCjg7rD4"
      },
      "outputs": [],
      "source": [
        "## Replace this with Cap1 Instance url\n",
        "WANDB_HOST = \"https://wandb.cloud.capitalone.com\" #@param\n",
        "# Equivalent to running \"wandb login\" in your shell\n",
        "\n",
        "wandb.login(host= WANDB_HOST)\n",
        "\n",
        "#\n",
        "# Note that https://api.wandb.ai is the default and points to the publicly hosted\n",
        "# app.\n",
        "#\n",
        "# Alternative you can configure this with environment variables:\n",
        "# export WANDB_API_KEY=\"<your-api-key>\"\n",
        "# export WANDB_BASE_URL=\"<your-wandb-endpoint>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zer98IpzJ-Df"
      },
      "source": [
        "Calling `wandb login` or `wandb.login` will write your API key to your `~/.netrc` file. __To authenticate the client in a headless job on the cloud, you will definitely want to use the `WANDB_API_KEY` environment variable__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1HvXvpFvwl0"
      },
      "source": [
        "**Default Destination:** When a user signs up to the instance and joins a team, wandb will automatically write runs this team. This setting is controlled directly through your settings and can be updated by\n",
        "\n",
        "*   Visiting https://<host-url>/settings\n",
        "*   Look for `Default Team` section\n",
        "*   Updating `Default location to create new projects` to entity of choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4Tn2Aj0FcIw"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "WANDB_ENTITY = 'wb-new-user-training-20251008' #@param #Point to a team you are a member of!\n",
        "WANDB_PROJECT = \"workshop_wandb_intro\" #@param\n",
        "YOUR_NAME = \"uma\" #@param #We will use this for our filtering and grouping to make it easy for your to identify your runs in the project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxfnAIEQ4gHd"
      },
      "source": [
        "### Log Dataframes of Media\n",
        "\n",
        "You can also log `pandas.DataFrame` objects with `.log`! These will be converted into a `wandb.Table` (docs) and interactievly displayed inside of W&B.\n",
        "\n",
        "Note: One of the most powerful features of `wandb.Table`s is that you can include any `wandb` type as a cell value! This includes, images, plots, videos, audio... almost anything ü§©\n",
        "\n",
        "Below we will use a the Oxford-IIIT Pet Dataset of 37 different pet breeds along with corresponding segmentation masks provided in the annotations for logging media example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mhxhSv44igt"
      },
      "outputs": [],
      "source": [
        "!curl -SL -qq https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz > images.tar.gz\n",
        "!curl -SL -qq https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz > annotations.tar.gz\n",
        "!tar -xzf images.tar.gz\n",
        "!tar -xzf annotations.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU3nYPr04j3n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wandb\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uK7GLQB4lfh"
      },
      "outputs": [],
      "source": [
        "#Utility functions\n",
        "\n",
        "# Function to load an image and mask\n",
        "def load_image_and_mask(image_path, mask_path):\n",
        "    image = np.array(Image.open(image_path))\n",
        "    mask = np.array(Image.open(mask_path))\n",
        "    return image, mask\n",
        "\n",
        "# Function to create W&B mask overlay\n",
        "def wb_mask(image, mask):\n",
        "    return wandb.Image(image, masks={\"predictions\": {\"mask_data\": mask}}, caption=\"Segmentation Image\")\n",
        "\n",
        "def log_single_images(path_img, path_lbl):\n",
        "  # Single Image and Mask Logging\n",
        "  image_path = path_img / 'Abyssinian_1.jpg'\n",
        "  mask_path = path_lbl / 'Abyssinian_1.png'\n",
        "  image_np, mask_np = load_image_and_mask(image_path, mask_path)\n",
        "  return image_np, mask_np\n",
        "\n",
        "def wandb_table_multiple_imags(path_img,path_lbl, num_images):\n",
        "    table = wandb.Table(columns=[\"ID\", \"Original Image\", \"Image with Mask\"])\n",
        "    # Multiple Images Logging in a Table\n",
        "    table = wandb.Table(columns=[\"ID\", \"Original Image\", \"Image with Mask\"])\n",
        "\n",
        "    # Log first X images and their masks to the table\n",
        "    for each in os.listdir(path_img)[:num_images]:  # limiting to first X images\n",
        "        image_path = path_img / each\n",
        "        mask_path = path_lbl / f'{Path(each).stem}.png'  # Adjust to match mask filenames\n",
        "        image_np, mask_np = load_image_and_mask(image_path, mask_path)\n",
        "\n",
        "        # Create mask overlay using W&B\n",
        "        mask_img = wb_mask(image_np, mask_np)\n",
        "\n",
        "        # Add image path, original image, and image with mask to the table\n",
        "        table.add_data(str(image_path), wandb.Image(image_np), mask_img)\n",
        "\n",
        "    return table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cALg7cQV4pCV"
      },
      "source": [
        "Logging a single image and a table of images with segmentation masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfH-0cx24ofw"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    entity = WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    group=YOUR_NAME,\n",
        "    name=\"logging_rich_media\",\n",
        "    )\n",
        "\n",
        "# Define paths\n",
        "path_img = Path('images')\n",
        "path_lbl = Path('annotations/trimaps')\n",
        "\n",
        "image_np, mask_np = log_single_images(path_img, path_lbl)\n",
        "\n",
        "# Log single image and segmentation mask\n",
        "run.log({\n",
        "    \"input_image\": wandb.Image(image_np, caption=\"Input Image\"),\n",
        "    \"segmentation_mask\": wb_mask(image_np, mask_np)\n",
        "})\n",
        "\n",
        "#Log tables of images and segmentation mask\n",
        "image_tables = wandb_table_multiple_imags(path_img, path_lbl, 30)\n",
        "\n",
        "# Log the table\n",
        "run.log({\"Segmentation Table\": image_tables})\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awd2IAIa8T17"
      },
      "source": [
        "### Log Sequences of Media\n",
        "\n",
        "If you periodically call `run.log` to log a number (for example, loss), Weights & Biases will automatically render a line plot showing the change in that value over time (a loss curve). You can also log media under a key more than once over the course of an experiment, in which case Weights & Biases will display that media with a step slider so you can scrub over the course of the experiment and see how it changed. This is particularly useful for seeing how model predictions and visualizations of model performance (e.g. a precision/recall curve) change over time. In the example below, we log a `wandb.Image` repeatedly just to demonstrate how this works. Below is an example of doing the same with audio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IFsGTdP8Y6l"
      },
      "outputs": [],
      "source": [
        "%%sh\n",
        "curl https://parade.com/.image/t_share/MTkwNTgwOTUyNjU2Mzg5MjQ1/albert-einstein-quotes-jpg.jpg > image.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Gp6oFiZ8cGC"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageFilter\n",
        "import pandas as pd\n",
        "# Load image with pillow, resize to 512 square\n",
        "im = Image.open(\"./image.jpg\").resize((512, 512))\n",
        "images = []\n",
        "with wandb.init(project = WANDB_PROJECT) as run:\n",
        "\n",
        "  for step in range(10):\n",
        "\n",
        "    # Log image\n",
        "    images.append( (step, wandb.Image(im)))\n",
        "    run.log({\"image\": wandb.Image(im)})\n",
        "\n",
        "    # Apply small Gaussian blur\n",
        "    im = im.filter(ImageFilter.GaussianBlur(radius=1.5))\n",
        "\n",
        "  # Also log the images + associated logging step to a W&B Table\n",
        "  run.log({ \"images_df\": pd.DataFrame( images, columns = [\"step\", \"images\"])})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REWlYGW0INyL"
      },
      "source": [
        "# Artifacts\n",
        "\n",
        "Use W&B Artifacts to track and version data as the inputs and outputs of your W&B Runs. In addition to logging hyperparameters, metadata, and metrics to a run, you can use an artifact to log the dataset used to train the model as input and the resulting model checkpoints as outputs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etKMjkIWz45u"
      },
      "source": [
        "For this demo, we are going to go through the workflow of\n",
        "1. Creating a dataset\n",
        "2. Logging it to wandb\n",
        "3. Processing that dataset\n",
        "4. Logging the processed data to wandb\n",
        "5. Conducting model training\n",
        "6. Viewing the entire lineage of this process in the wandb UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr0H-G_9nDoQ"
      },
      "source": [
        "## Create a Dataset\n",
        "Let's create some datasets that we can work with in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ho7TWxacm8k3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "directory = \"dataset\"\n",
        "os.makedirs(directory, exist_ok=True)\n",
        "file1, file2 = os.path.join(directory, \"file1.csv\"), os.path.join(directory, \"file2.csv\")\n",
        "\n",
        "def generate_dummy_data(num_samples):\n",
        "    data = [\n",
        "        np.random.normal(50, 10, num_samples),\n",
        "        np.random.randint(1, 100, num_samples),\n",
        "        np.random.choice(['A', 'B', 'C', 'D'], num_samples),\n",
        "        np.random.uniform(0.0, 1.0, num_samples)\n",
        "    ]\n",
        "    return zip(*data)\n",
        "\n",
        "def save_to_csv(file, data):\n",
        "    with open(file, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['feature1', 'feature2', 'feature3', 'feature4'])\n",
        "        writer.writerows(data)\n",
        "\n",
        "num_samples = 100\n",
        "save_to_csv(file1, generate_dummy_data(num_samples))\n",
        "save_to_csv(file2, generate_dummy_data(num_samples))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESkuFmftqiX9"
      },
      "source": [
        "The general workflow for creating an Artifact is:\n",
        "\n",
        "1. Intialize a run.\n",
        "2. Create an Artifact.\n",
        "3. Add a any files, directories, or pointers to the new Artifact that you want to track and version.\n",
        "4. Log the artifact in the W&B platform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1750nmI7tw8Y"
      },
      "source": [
        "See the [Artifacts Reference guide](https://www.google.com/url?q=https%3A%2F%2Fdocs.wandb.ai%2Fref%2Fpython%2Fartifact) for more information and other commonly used arguments, including how to store additional metadata.\n",
        "\n",
        "Each time the above `log_artifact` is executed, wandb will create a new version of the Artifact within Weights & Biases if the underlying data has changed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pbfQ-F2yxSB"
      },
      "source": [
        "## Logging this dataset artifact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyX4w4jAqfA2"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT, job_type='log_dataset')\n",
        "\n",
        "artifact = wandb.Artifact(f\"my_first_artifact_{YOUR_NAME}\", type=\"dataset\")\n",
        "# the below will add two individual files to the artifact.\n",
        "artifact.add_file(local_path=f\"{directory}/file1.csv\")\n",
        "artifact.add_file(local_path=f\"{directory}/file2.csv\")\n",
        "\n",
        "# or the below if you wanted to add the entire directory contents.\n",
        "artifact.add_dir(local_path=f\"{directory}\")\n",
        "# explictly log the artifact to Weights & Biases.\n",
        "run.log_artifact(artifact)\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trM9_rvT3Kmp"
      },
      "source": [
        "## Processing and Consuming the dataset Artifact"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0BOwIDM3SQz"
      },
      "source": [
        "When you want to use a specific version of an Artifact in a downstream task, you can specify the specific version you would like to use via either `v0`, `v1`, `v2` and so on, or via specific aliases you may have added. The latest alias always refers to the most recent version of the Artifact logged.\n",
        "\n",
        "The proceeding code snippet specifies that the W&B Run will use an artifact called my_first_artifact with the alias latest. We will take a step to preprocess our dataset and relog it to wandb, so we can see the lineage up until this point:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDr-prUD_7hN"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT, job_type='process_dataset')\n",
        "artifact = run.use_artifact(artifact_or_name=f\"my_first_artifact_{YOUR_NAME}:latest\") # this creates a reference within Weights & Biases that this artifact was used by this run.\n",
        "path = artifact.download() # this downloads the artifact from Weights & Biases to your local system where the code is executing.\n",
        "\n",
        "print(f\"Data directory located at {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzWv3wEfWyjf"
      },
      "outputs": [],
      "source": [
        "processed_directory = \"processed_dataset\"\n",
        "os.makedirs(processed_directory, exist_ok=True)\n",
        "file1, file2 = os.path.join(directory, \"file1_processed.csv\"), os.path.join(directory, \"file2_processed.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY6X_0PuVkSN"
      },
      "outputs": [],
      "source": [
        "# Step 2: Function to process and save the modified CSV data\n",
        "def process(input_file_path, output_file_path):\n",
        "    modified_data = []\n",
        "    with open(input_file_path, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        headers = next(reader)  # Skip headers\n",
        "        for row in reader:\n",
        "            # Example modification: Adjust feature1 by adding a constant\n",
        "            row[0] = str(float(row[0]) + 10)  # Modify feature1\n",
        "            modified_data.append(row)\n",
        "\n",
        "    # Save the modified data to the output path\n",
        "    with open(output_file_path, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(headers)  # Write the headers back\n",
        "        writer.writerows(modified_data)\n",
        "\n",
        "# Apply modification to the CSV files\n",
        "process(os.path.join(path, \"file1.csv\"), os.path.join(processed_directory, \"file1_processed.csv\"))\n",
        "process(os.path.join(path, \"file2.csv\"), os.path.join(processed_directory, \"file2_processed.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYfyaZpTTA0R"
      },
      "outputs": [],
      "source": [
        "# Step 4: Create a new artifact to store the modified data\n",
        "processed_artifact = wandb.Artifact(\n",
        "    f\"my_processed_artifact_{YOUR_NAME}\",\n",
        "    type=\"processed_dataset\"\n",
        ")\n",
        "\n",
        "# Add the modified CSV files to the new artifact\n",
        "processed_artifact.add_file(local_path=f\"{processed_directory}/file1_processed.csv\")\n",
        "processed_artifact.add_file(local_path=f\"{processed_directory}/file1_processed.csv\")\n",
        "\n",
        "# or the below if you wanted to add the entire directory contents.\n",
        "processed_artifact.add_dir(local_path=f\"{processed_directory}\")\n",
        "\n",
        "# Step 5: Log the processed artifact\n",
        "run.log_artifact(processed_artifact)\n",
        "\n",
        "# Finish the run\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnZWCPQoRDiO"
      },
      "source": [
        "## Using our dataset during model training and logging model checkpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHu4gt7LIbjL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "run = wandb.init(\n",
        "    entity = WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    group=YOUR_NAME,\n",
        "    job_type = \"training\",\n",
        "    config={'param': 42}\n",
        ")\n",
        "\n",
        "# Use our processed dataset in our training run\n",
        "# this creates a reference within Weights & Biases that this artifact was used by this run\n",
        "artifact = run.use_artifact(artifact_or_name=f\"my_processed_artifact_{YOUR_NAME}:latest\")\n",
        "\n",
        "#Save simple neural network model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 50)\n",
        "        self.fc2 = nn.Linear(50, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleModel()\n",
        "\n",
        "# doing some dummy logging here\n",
        "for i in range(5):\n",
        "  run.log({\"acc\": random.random()})\n",
        "\n",
        "# Save the model and log to artifacts\n",
        "model_path = \"simple_model.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "# Log the model as an artifact\n",
        "art = wandb.Artifact(name=f\"simple-model-{YOUR_NAME}\", type=f\"model\")\n",
        "art.add_file(model_path)\n",
        "run.log_artifact(art)\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxhnts5G-ZWe"
      },
      "source": [
        "For more information on ways to customize your Artifact download, including via the command line, see the [Download and Usage guide](https://docs.wandb.ai/guides/artifacts/download-and-use-an-artifact)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv25K2MLztOr"
      },
      "source": [
        "## How can we see which experiments used a particular dataset?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxKTeSO063y-"
      },
      "source": [
        "If we want to see which wandb runs consumed a specific dataset, we can do this two ways:\n",
        "1. Viewing the lineage in the UI\n",
        "2. Programmatically accessing which run names and ids consumed a specific dataset (below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pta-CyiIzw4P"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    entity = WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        ")\n",
        "\n",
        "artifact = run.use_artifact(artifact_or_name=f\"my_processed_artifact_{YOUR_NAME}:latest\")\n",
        "\n",
        "for run in artifact.used_by():\n",
        "  print(run.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9egMJyHQsG1j"
      },
      "source": [
        "## Update Artifact version metadata\n",
        "You can update the description, metadata, and alias of an artifact on the W&B platform during or outside a W&B Run.\n",
        "\n",
        "This example changes the description of the my_first_artifact artifact inside a run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqJeGfv8sMcF"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT)\n",
        "artifact = run.use_artifact(artifact_or_name=f\"my_first_artifact_{YOUR_NAME}:latest\")\n",
        "artifact.description = \"This is an edited description.\"\n",
        "artifact.metadata = {\"source\": \"local disk\", \"internal data owner\": \"platform team\"}\n",
        "artifact.save()  # persists changes to an Artifact's properties\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-1UwWmJaGns"
      },
      "source": [
        "## Code Snippet for Logging Reference Artifacts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbeuzUScSseM"
      },
      "source": [
        "Artifacts currently support the following URI schemes:\n",
        "\n",
        "* **http(s)://:** A path to a file accessible over HTTP. The artifact will track checksums in the form of etags and size metadata if the HTTP server supports the ETag and Content-Length response headers.\n",
        "* **s3://:** A path to an object or object prefix in S3. The artifact will track checksums and versioning information (if the bucket has object versioning enabled) for the referenced objects. Object prefixes are expanded to include the objects under the prefix, default up to 100,000 objects.\n",
        "* **gs://:** A path to an object or object prefix in GCS. The artifact will track checksums and versioning information (if the bucket has object versioning enabled) for the referenced objects. Object prefixes are expanded to include the objects under the prefix, default up to 100,000 objects.\n",
        "See below for an example of reference local files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aR6RxpxSr9Y"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                project=WANDB_PROJECT,\n",
        "                job_type=\"upload-references\")\n",
        "artifact = wandb.Artifact(name=f\"local-file-references_{YOUR_NAME}\", type=\"reference-dataset\")\n",
        "artifact.add_reference(\"file:///content/sample_data\", checksum=True)\n",
        "run.log_artifact(artifact)\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozxtC0WVZGB2"
      },
      "source": [
        "## **Artifacts Time-to-live (TTL)**\n",
        "\n",
        "W&B Artifacts supports setting time-to-live policies on each version of an Artifact. The following examples show the use TTL policy in a common Artifact logging workflow. We'll cover:\n",
        "\n",
        "* Setting a TTL policy when creating an Artifact\n",
        "* Retroactively setting TTL for a specific Artifact aliases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoNcRyerZIrY"
      },
      "source": [
        "## Setting TTL on New Artifacts\n",
        "Below we create two new Artifacts from the colab provided sample_data\n",
        "- mnist_test.csv\n",
        "- mnist_train_small.csv\n",
        "\n",
        "Upload them as artifacts files to artifact of type `mnist_dataset` and assign them a TTL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFpn0tBRZK8L"
      },
      "outputs": [],
      "source": [
        "from datetime import timedelta\n",
        "\n",
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                project=WANDB_PROJECT,\n",
        "                job_type=\"raw-data\")\n",
        "\n",
        "raw_mnist_train = wandb.Artifact(\n",
        "    f\"mnist_train_small_{YOUR_NAME}\",\n",
        "    type=\"mnist_dataset\",\n",
        "    description=\"Small MNIST Training Set\"\n",
        ")\n",
        "\n",
        "raw_mnist_train.add_file(\"sample_data/mnist_train_small.csv\")\n",
        "raw_mnist_train.ttl = timedelta(days=10)\n",
        "run.log_artifact(raw_mnist_train, aliases=[\"small\", \"mnist\", \"train\"])\n",
        "\n",
        "raw_mnist_test = wandb.Artifact(\n",
        "    f\"mnist_test_small_{YOUR_NAME}\",\n",
        "    type=\"mnist_dataset\",\n",
        "    description=\"Small MNIST Test Set\"\n",
        ")\n",
        "\n",
        "raw_mnist_test.add_file(\"sample_data/mnist_test.csv\")\n",
        "raw_mnist_test.ttl = timedelta(days=10)\n",
        "run.log_artifact(raw_mnist_test, aliases=[\"small\", \"mnist\", \"test\"])\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ8JNB_nZQmS"
      },
      "source": [
        "# Full pipeline - data upload, model training, linking the best model to registry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCcKODegaSe1"
      },
      "source": [
        "The below pipeline includes:\n",
        "\n",
        "* Data Versioning: The Heart Disease dataset is split into training, validation, and test sets, each logged as a W&B artifact for easy tracking and reproducibility. The training dataset we linked to our dataset registry\n",
        "\n",
        "* Model Training: A neural network is trained on the training set, with performance monitored on the validation set. The best model version is saved and versioned as a W&B artifact which is then linked to our model registry\n",
        "\n",
        "* What It Showcases:\n",
        "How to use W&B for seamless data and model versioning.\n",
        "Best practices for tracking model performance during training and evaluation.\n",
        "Efficient and reproducible ML workflows in a production-ready environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rMVsil1ZQ4w"
      },
      "outputs": [],
      "source": [
        "COLLECTION_NAME = \"heart-disease\" #@param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xlFicFMDgnq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "\n",
        "# Load the Heart Disease dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
        "columns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
        "           \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cn-qshIaDgnq"
      },
      "outputs": [],
      "source": [
        "# Data download and processing\n",
        "raw_data = !CURL https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
        "clean_rows = [\n",
        "    row\n",
        "    for row in raw_data\n",
        "    if row.strip()                  # drop the empty string\n",
        "       and \"--:--:--\" not in row    # progress-bar lines\n",
        "       and \"%\" not in row           # header / percent lines\n",
        "       and \"Dload\" not in row       # second header line\n",
        "]\n",
        "\n",
        "csv = '\\n'.join(clean_rows)\n",
        "data = pd.read_csv(StringIO(csv), header=None, names=columns)\n",
        "\n",
        "# Replace missing values ('?') with NaN and drop rows with NaN values\n",
        "data.replace('?', np.nan, inplace=True)\n",
        "data = data.dropna().astype(float)\n",
        "\n",
        "# Convert target variable: 0 = no heart disease, 1 = presence of heart disease\n",
        "data['target'] = (data['target'] > 0).astype(int)\n",
        "\n",
        "# Shuffle the dataset to ensure random distribution\n",
        "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Perform a train/validation/test split (60/20/20)\n",
        "train_size = int(0.6 * len(data))\n",
        "val_size = int(0.2 * len(data))\n",
        "test_size = len(data) - train_size - val_size\n",
        "\n",
        "train_data = data[:train_size]\n",
        "val_data = data[train_size:train_size + val_size]\n",
        "test_data = data[train_size + val_size:]\n",
        "\n",
        "# Save the entire dataset as a CSV file\n",
        "data.to_csv(\"heart_disease_full_dataset.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TJZOIJRal_k"
      },
      "source": [
        "The following function is used to save a dataset to a file and log it as an artifact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6y_SYLwampB"
      },
      "outputs": [],
      "source": [
        "# Simple function to save log dataset artifacts\n",
        "def save_and_log_dataset(data, filename, artifact_name, aliases):\n",
        "    # Save the dataset as a CSV file\n",
        "    data.to_csv(filename, index=False)\n",
        "\n",
        "    # Create and log the dataset artifact\n",
        "    dataset_artifact = wandb.Artifact(name=artifact_name, type='dataset')\n",
        "    dataset_artifact.add_file(filename)\n",
        "    wandb.log_artifact(dataset_artifact, aliases=aliases)\n",
        "\n",
        "    return dataset_artifact"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPNJrBpUaxaI"
      },
      "source": [
        "The following cell is going to save our train, validation, and test datasets as artifacts, as well as link our training dataset to our Dataset registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pag6Dbjsayyz"
      },
      "outputs": [],
      "source": [
        "#Upload data to wandb\n",
        "\n",
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                project=WANDB_PROJECT,\n",
        "                group = YOUR_NAME,\n",
        "                job_type=\"heart-disease-data-uploads\",\n",
        "                name = f\"heart_disease_data_uploads_{YOUR_NAME}\",\n",
        "                tags = [\"data-upload\"]\n",
        "                )\n",
        "\n",
        "# Save and log the entire dataset\n",
        "full_artifact = save_and_log_dataset(data, \"heart_disease_full_dataset.csv\", f'heart_disease_full_dataset_{YOUR_NAME}', [\"initial_commit\", \"complete_dataset\"])\n",
        "\n",
        "# Save and log the training dataset\n",
        "train_artifact = save_and_log_dataset(train_data, \"heart_disease_train_dataset.csv\", f'heart_disease_train_dataset_{YOUR_NAME}', [\"initial_commit\", \"train_split\"])\n",
        "\n",
        "# Save and log the validation dataset\n",
        "val_artifact = save_and_log_dataset(val_data, \"heart_disease_val_dataset.csv\", f'heart_disease_validation_dataset_{YOUR_NAME}', [\"initial_commit\", \"validation_split\"])\n",
        "\n",
        "# Save and log the test dataset\n",
        "test_artifact = save_and_log_dataset(test_data, \"heart_disease_test_dataset.csv\", f'heart_disease_test_dataset_{YOUR_NAME}', [\"initial_commit\", \"test_split\"])\n",
        "\n",
        "\n",
        "#Log all dataset to W&B tables for visual analysis\n",
        "run.log({f\"train_data_table_{YOUR_NAME}\": wandb.Table(dataframe=train_data),\n",
        "           f\"test_data_table_{YOUR_NAME}\": wandb.Table(dataframe=test_data),\n",
        "           f\"validation_data_table_{YOUR_NAME}\": wandb.Table(dataframe=val_data)})\n",
        "\n",
        "# Linking Training Dataset to collection in dataset registry\n",
        "target_path = f\"WandB-Intro-Workshop-Registry-dataset/{COLLECTION_NAME}\"\n",
        "\n",
        "# this line will only work if you are logging to a team entity - which has access to Registry\n",
        "run.link_artifact(\n",
        "  artifact=train_artifact,\n",
        "  target_path= target_path\n",
        ")\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR42aD8la41X"
      },
      "source": [
        "The following cell is going to pull the latest training dataset from our training dataset registry so we can start training on it.\n",
        "\n",
        "During training, we save our model checkpoints as artifacts, but we're going to promote our best model from our training to our model registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPd14KnQa6eN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbrZUSXZa7rz"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#simple function for loading artifacts from wandb\n",
        "def load_and_split_data(entity, project, artifact_name, your_name, split_name):\n",
        "\n",
        "    artifact_full_name = f'{entity}/{project}/{artifact_name}_{your_name}:latest'\n",
        "    artifact = wandb.use_artifact(artifact_full_name, type='dataset')\n",
        "    artifact_dir = artifact.download()#If you are using local version of artifact, you can simple utilize wandb.use_artifact() only instead of downloading to associated lineage to the artifact\n",
        "\n",
        "    data = pd.read_csv(f\"{artifact_dir}/heart_disease_{split_name}_dataset.csv\")\n",
        "\n",
        "    X = torch.tensor(data.drop(\"target\", axis=1).values, dtype=torch.float32)\n",
        "    y = torch.tensor(data[\"target\"].values, dtype=torch.float32)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# Initialize training run\n",
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                project=WANDB_PROJECT,\n",
        "                group = YOUR_NAME,\n",
        "                job_type=\"heart-disease-training\",\n",
        "                name = f\"heart_disease_training_validation_{YOUR_NAME}\"\n",
        "                )\n",
        "\n",
        "\n",
        "# Explicitly accessing Training data\n",
        "artifact = run.use_artifact('WandB-Intro-Workshop-Registry-dataset/heart-disease:latest', type='dataset')\n",
        "artifact_dir = artifact.download()\n",
        "\n",
        "data = pd.read_csv(f\"{artifact_dir}/heart_disease_train_dataset.csv\")\n",
        "\n",
        "X_train = torch.tensor(data.drop(\"target\", axis=1).values, dtype=torch.float32)\n",
        "y_train = torch.tensor(data[\"target\"].values, dtype=torch.float32)\n",
        "\n",
        "# Load validation data\n",
        "X_val, y_val = load_and_split_data(WANDB_ENTITY, WANDB_PROJECT, 'heart_disease_validation_dataset', YOUR_NAME, 'val')\n",
        "\n",
        "# Define a simple neural network model\n",
        "class HeartDiseaseModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(HeartDiseaseModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.bn3 = nn.BatchNorm1d(32)\n",
        "        self.fc4 = nn.Linear(32, 16)\n",
        "        self.fc5 = nn.Linear(16, 1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = torch.sigmoid(self.fc5(x))\n",
        "        return x\n",
        "\n",
        "model = HeartDiseaseModel(input_size=X_train.shape[1])\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "best_performance = float('inf')\n",
        "version = 1\n",
        "\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train).squeeze()\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate and log training accuracy\n",
        "    predictions = (outputs >= 0.5).float()\n",
        "    train_accuracy = (predictions == y_train).float().mean().item()\n",
        "\n",
        "    run.log({\"train/epoch\": epoch, \"train/train_loss\": loss.item(), \"train/train_accuracy\": train_accuracy})\n",
        "\n",
        "    # Evaluate the model on validation set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val).squeeze()\n",
        "        val_loss = criterion(val_outputs, y_val).item()\n",
        "\n",
        "        # Calculate and log validation accuracy\n",
        "        val_predictions = (val_outputs >= 0.5).float()\n",
        "        val_accuracy = (val_predictions == y_val).float().mean().item()\n",
        "\n",
        "        run.log({\"val/val_loss\": val_loss, \"val/val_accuracy\": val_accuracy})\n",
        "\n",
        "        if val_loss < best_performance:\n",
        "            best_performance = val_loss\n",
        "            model_path = f\"heart_disease_model_v{version}.pth\"\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            artifact = wandb.Artifact(name=f'heart_disease_model_{YOUR_NAME}', type='model')\n",
        "            artifact.add_file(model_path)\n",
        "            wandb.log_artifact(artifact, aliases=[f\"v{version}\", \"best\"])\n",
        "            version += 1\n",
        "\n",
        "# linking the best model to collection in model registry\n",
        "target_path = f\"WandB-Intro-Workshop-Registry-model/{COLLECTION_NAME}\"\n",
        "\n",
        "# Giving it the production alias since this is our best model\n",
        "# this line will only work if you are logging to a team entity\n",
        "run.link_artifact(\n",
        "  artifact=artifact,\n",
        "  target_path= target_path,\n",
        "  aliases=[\"production\"]\n",
        ")\n",
        "\n",
        "run.finish()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Wv25K2MLztOr",
        "9egMJyHQsG1j",
        "K-1UwWmJaGns"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
