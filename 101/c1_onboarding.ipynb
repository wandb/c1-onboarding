{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t1WrY34u_Wi"
      },
      "source": [
        "# Weights & Biases 101 ü•æüèïÔ∏è\n",
        "\n",
        "This notebook is intended to show you how to track your machine learning experiments using [Weights & Biases](https://wandb.ai).\n",
        "\n",
        "Weights & Biases has two major components: a python client named `wandb` ü™Ñüêù  and a web application that allows you to store, query, visualize, and share metadata from your machine learning experiments, e.g. loss curves, evaluation metrics, model predictions... you can `.log` *just about* anything.\n",
        "\n",
        "The client is open source and you can find the [source code on Github](http://github.com/wandb/wandb)! ‚≠ê\n",
        "\n",
        "The first step on our journey is to install the client, which is as easy as:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i0N-GAivDsw"
      },
      "source": [
        "## ü™Ñ Install `wandb` library and login\n",
        "\n",
        "\n",
        "The first step on our journey is to install the client, which is as easy as:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6yGlDervEh8"
      },
      "outputs": [],
      "source": [
        "!pip install c1_aem_sdk -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojUNT0WhGLjc"
      },
      "outputs": [],
      "source": [
        "!pip install \"numpy<1.26.4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZ6505_yammL"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E83xgBm8yjn"
      },
      "source": [
        "## Log in to W&B\n",
        "- You can explicitly login using `wandb login` or `wandb.login()` (See below)\n",
        "- Alternatively you can set environment variables. There are several env variables which you can set to change the behavior of W&B logging. The most important are:\n",
        "    - `WANDB_API_KEY` - find this in your \"Settings\" section under your profile\n",
        "    - `WANDB_BASE_URL` - this is the url of the W&B server\n",
        "- Find your API Token in \"Profile\" -> \"Setttings\" in the W&B App\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2oj15Y_Eyt5"
      },
      "outputs": [],
      "source": [
        "import c1_aem_sdk\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsNzSCjg7rD4"
      },
      "outputs": [],
      "source": [
        "## Replace this with Cap1 Instance url\n",
        "WANDB_HOST = \"<cap1_instance_url>\" #@param\n",
        "# Equivalent to running \"wandb login\" in your shell\n",
        "\n",
        "wandb.login(host= WANDB_HOST)\n",
        "\n",
        "#\n",
        "# Note that https://api.wandb.ai is the default and points to the publicly hosted\n",
        "# app.\n",
        "#\n",
        "# Alternative you can configure this with environment variables:\n",
        "# export WANDB_API_KEY=\"<your-api-key>\"\n",
        "# export WANDB_BASE_URL=\"<your-wandb-endpoint>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zer98IpzJ-Df"
      },
      "source": [
        "Calling `wandb login` or `wandb.login` will write your API key to your `~/.netrc` file. __To authenticate the client in a headless job on the cloud, you will definitely want to use the `WANDB_API_KEY` environment variable__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1HvXvpFvwl0"
      },
      "source": [
        "**Default Destination:** When a user signs up to the instance and joins a team, wandb will automatically write runs this team. This setting is controlled directly through your settings and can be updated by\n",
        "\n",
        "*   Visiting https://<host-url>/settings\n",
        "*   Look for `Default Team` section\n",
        "*   Updating `Default location to create new projects` to entity of choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "J4Tn2Aj0FcIw"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "WANDB_ENTITY = '<team-name>' #@param #Point to a team you are a member of!\n",
        "WANDB_PROJECT = \"workshop_wandb_intro\" #@param\n",
        "YOUR_NAME = \"uma\" #@param #We will use this for our filtering and grouping to make it easy for your to identify your runs in the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3F58BBJwR-B"
      },
      "outputs": [],
      "source": [
        "# Track hyperparameters and run metadata\n",
        "WANDB_CONFIG = {\n",
        "    \"learning_rate\": 0.02,\n",
        "    \"architecture\": \"CNN\",\n",
        "    \"dataset\": \"CIFAR-100\",\n",
        "    \"epochs\": 10,\n",
        "}\n",
        "\n",
        "#Tags can be used to label runs with particular features that might not be obvious from the logged metrics\n",
        "WANDB_TAGS = [\"production\", \"baseline\"]\n",
        "\n",
        "\"\"\"\n",
        "The pattern of \"with wandb.init()...\" causes wandb.finish() to be called as\n",
        "soon as we leave the with block. This is especially useful when you have a script\n",
        "or notebook that initializes multiple runs that you want to track separately.\n",
        "\"\"\"\n",
        "\n",
        "# Launch 5 simulated experiments\n",
        "total_runs = 5\n",
        "for exp in range(total_runs):\n",
        "  #Start a new run to track this script\n",
        "  run = wandb.init(\n",
        "      entity=WANDB_ENTITY,       # Set the Entity to destination team\n",
        "      project=WANDB_PROJECT,     # Set the project where this run will be logged\n",
        "      name=f\"experiment_{exp}\",  # We pass a run name (otherwise it‚Äôll be randomly assigned, like sunshine-lollypop-10)\n",
        "      group=f\"production_group_{YOUR_NAME}\", # For organizing runs (e.g. distributed training)\n",
        "      job_type='training',\t\t   # For organizing runs (e.g. preprocessing vs. training)\n",
        "      config=WANDB_CONFIG,       # For saving experiment parameters\n",
        "      tags= WANDB_TAGS           # For organizing runs together or applying temporary labels\n",
        "      )\n",
        "\n",
        "  # This simple block simulates a training loop logging metrics\n",
        "  epochs = 10\n",
        "  offset = random.random() / 5\n",
        "  for epoch in range(2, epochs):\n",
        "      acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
        "      loss = 2 ** -epoch + random.random() / epoch + offset\n",
        "      # Scalar metrics can be logged by passing them in to  run.log as a dictionary with a name.\n",
        "      run.log({\"acc\": acc, \"loss\": loss})\n",
        "\n",
        "  # Mark the run as finished\n",
        "  run.finish()\n",
        "\n",
        "run_id_for_api = run.id #will be utilized when covering api usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKxnEXB8qX9I"
      },
      "source": [
        "### Organizing logged metrics?\n",
        "\n",
        "We treat / as a separator for organizing logged panels in the W&B UI. By default, the component of the logged item's name before a / is used to define a group of panel called a \"Panel Section\".\n",
        "\n",
        "```\n",
        "run.log({\"val/loss\": 1.1, \"val/acc\": 0.3})\n",
        "run.log({\"train/loss\": 0.1, \"train/acc\": 0.94})\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAcJDrCvDCcP"
      },
      "source": [
        "\n",
        "### Wandb Step & Custom Metrics\n",
        "\n",
        "1. **What is the `wandb Step`?**\n",
        "    \n",
        "    The wandb step tracks the progression of your training or experimentation process. Steps are monotonically increasing and increment everytime you call `run.log({key:val})`. To keep your metrics step synchornized, we recommend you bundle your metrics into the same `log()` call.\n",
        "\n",
        "If interested in using a custom x axis that is not wandb step use `define_metric` to set a custom x axis.Custom x-axes are useful in contexts where you need to log to different time steps in the past during training, asynchronously. Example below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZFVKzfEDBrk"
      },
      "outputs": [],
      "source": [
        "#Organizing logged metrics\n",
        "run = wandb.init(\n",
        "    entity = WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    group = YOUR_NAME,\n",
        "    name = \"organized_metrics_run\"\n",
        ")\n",
        "\n",
        "# Define a custom x axis metric\n",
        "wandb.define_metric(\"custom/custom_step\")\n",
        "\n",
        "# Define which metrics to plot against that x-axis\n",
        "wandb.define_metric(\"custom/unique_metric\", step_metric='custom/custom_step')\n",
        "\n",
        "epochs = 100\n",
        "offset = random.random() / 5\n",
        "for epoch in range(2, epochs):\n",
        "    val_acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
        "    val_loss = 2 ** -epoch + random.random() / epoch + offset\n",
        "    train_acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
        "    train_loss = 2 ** -epoch + random.random() / epoch + offset\n",
        "    run.log({\"train/acc\": train_acc,\n",
        "              \"train/loss\": train_loss,\n",
        "              \"val/acc\": val_acc,\n",
        "              \"val/loss\": val_loss,\n",
        "             \"custom/custom_step\": epoch**2,\n",
        "             \"custom/unique_metric\": 1/(epoch+1)\n",
        "             })\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKXlpgK0J4kF"
      },
      "source": [
        "## Run Resuming\n",
        "\n",
        "Resuming runs is a powerful feature that allows you to actively continue your trainings from where you left off. To resume runs, set the args `id` and `resume` when initializing the run, `wandb.init(id=<run-id>, resume=<val>)`. For more information on run resuming see our [resuming docs](https://docs.wandb.ai/guides/runs/resuming)\n",
        "\n",
        "| Argument | Description                                        | Run ID exists                                    | Run ID does not exist                                  | Use case                                           |\n",
        "|----------|----------------------------------------------------|--------------------------------------------------|-------------------------------------------------------|----------------------------------------------------|\n",
        "| `must`   | W&B must resume run specified by the run ID.       | W&B resumes run with the same run ID.            | W&B raises an error.                                  | Resume a run that must use the same run ID.        |\n",
        "| `allow`  | Allow W&B to resume run if run ID exists.          | W&B resumes run with the same run ID.            | W&B initializes a new run with specified run ID.      | Resume a run without overriding an existing run.   |\n",
        "| `never`  | Never allow W&B to resume a run specified by run ID. | W&B raises an error.                              | W&B initializes a new run with specified run ID.      | Prevent resumption and always start a new run.     |\n",
        "\n",
        "\n",
        "**Example Use Cases**\n",
        "\n",
        "**Long-Running Training Jobs:** When training a model on a large dataset that requires several days to complete, you might need to pause the job due to resource limits. Using run resume, you can restart the training process without losing progress.\n",
        "\n",
        "**Cluster Time Limits:** In environments where compute jobs are limited by wall-time (e.g., SLURM or other job schedulers), you can use the resume feature to pick up right where the job was terminated, making it easier to manage long-running experiments.\n",
        "\n",
        "**Experiment Interruptions:** In case of unexpected interruptions (e.g., hardware failure, power outage), the resume feature ensures that you can continue the experiment once the issue is resolved, preventing data loss and ensuring consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrUnaR6cJ4Ad"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    entity = WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    group = f\"{YOUR_NAME}\",\n",
        "    name= f\"exp_run_with_resume_{YOUR_NAME}\"\n",
        ")\n",
        "\n",
        "#Log 5 points of the metric\n",
        "for i in range(5):\n",
        "  run.log({\"metric1\": i})\n",
        "\n",
        "\n",
        "run.finish()\n",
        "\n",
        "#First inspect run result before executing the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrWVEL7hZbQb"
      },
      "outputs": [],
      "source": [
        "resumed_run = wandb.init(\n",
        "    entity = WANDB_ENTITY,\n",
        "    project = WANDB_PROJECT,\n",
        "    group = f\"{YOUR_NAME}\",\n",
        "    name=  f\"exp_run_with_resume_{YOUR_NAME}\",\n",
        "    id = run.id, # If you do not pass in an id, wandb will use last known run id.\n",
        "    resume = \"must\"\n",
        ")\n",
        "\n",
        "#log an additional 5 points to metric1 and log a new metric, metric2. Notice how metric2 begins at step 5!\n",
        "for i in range(5):\n",
        "  resumed_run.log({\"metric1\": i,\n",
        "             \"metric2\": i**2})\n",
        "resumed_run.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgRqnTWe4cg_"
      },
      "source": [
        "## Rich Media - Brief\n",
        "\n",
        "W&B supports images, video, audio, and more. Log rich media to explore your results and visually compare your runs, models, and datasets. Read on for examples and how-to guides.\n",
        "\n",
        "For a first list of loggable types check out [these docs](https://docs.wandb.ai/ref/python/data-types)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hmJOWiLudNi"
      },
      "source": [
        "### Logging a single image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od_neLVjuTOc"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Initialize a new W&B run\n",
        "run = wandb.init(project=WANDB_PROJECT, name=\"image_logging_example\")\n",
        "\n",
        "# Load or create an image (example using a random numpy array)\n",
        "image = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)  # Random 100x100 RGB image\n",
        "image_pil = Image.fromarray(image)\n",
        "\n",
        "# Log the image to W&B\n",
        "run.log({\"sample_image\": wandb.Image(image_pil)})\n",
        "\n",
        "# Finish the run\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRDDsZG6uovZ"
      },
      "source": [
        "To log an overlay, you'll need to provide a dictionary with the following keys and values to the `masks` keyword argument of `wandb.Image`:\n",
        "\n",
        "- one of two keys representing the image mask:\n",
        "  - \"mask_data\": a 2D NumPy array containing an integer class label for each pixel\n",
        "  - \"path\": (string) a path to a saved image mask file\n",
        "- \"class_labels\": (optional) a dictionary mapping the integer class labels in the image mask to their readable class names\n",
        "\n",
        "To log multiple masks, log a mask dictionary with multiple keys, as done below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxfnAIEQ4gHd"
      },
      "source": [
        "### Log Dataframes of Media\n",
        "\n",
        "You can also log `pandas.DataFrame` objects with `.log`! These will be converted into a `wandb.Table` (docs) and interactievly displayed inside of W&B.\n",
        "\n",
        "Note: One of the most powerful features of `wandb.Table`s is that you can include any `wandb` type as a cell value! This includes, images, plots, videos, audio... almost anything ü§©\n",
        "\n",
        "Below we will use a the Oxford-IIIT Pet Dataset of 37 different pet breeds along with corresponding segmentation masks provided in the annotations for logging media example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mhxhSv44igt"
      },
      "outputs": [],
      "source": [
        "!curl -SL -qq https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz > images.tar.gz\n",
        "!curl -SL -qq https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz > annotations.tar.gz\n",
        "!tar -xzf images.tar.gz\n",
        "!tar -xzf annotations.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU3nYPr04j3n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wandb\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uK7GLQB4lfh"
      },
      "outputs": [],
      "source": [
        "#Utility functions\n",
        "\n",
        "# Function to load an image and mask\n",
        "def load_image_and_mask(image_path, mask_path):\n",
        "    image = np.array(Image.open(image_path))\n",
        "    mask = np.array(Image.open(mask_path))\n",
        "    return image, mask\n",
        "\n",
        "# Function to create W&B mask overlay\n",
        "def wb_mask(image, mask):\n",
        "    return wandb.Image(image, masks={\"predictions\": {\"mask_data\": mask}}, caption=\"Segmentation Image\")\n",
        "\n",
        "def log_single_images(path_img, path_lbl):\n",
        "  # Single Image and Mask Logging\n",
        "  image_path = path_img / 'Abyssinian_1.jpg'\n",
        "  mask_path = path_lbl / 'Abyssinian_1.png'\n",
        "  image_np, mask_np = load_image_and_mask(image_path, mask_path)\n",
        "  return image_np, mask_np\n",
        "\n",
        "def wandb_table_multiple_imags(path_img,path_lbl, num_images):\n",
        "    table = wandb.Table(columns=[\"ID\", \"Original Image\", \"Image with Mask\"])\n",
        "    # Multiple Images Logging in a Table\n",
        "    table = wandb.Table(columns=[\"ID\", \"Original Image\", \"Image with Mask\"])\n",
        "\n",
        "    # Log first X images and their masks to the table\n",
        "    for each in os.listdir(path_img)[:num_images]:  # limiting to first X images\n",
        "        image_path = path_img / each\n",
        "        mask_path = path_lbl / f'{Path(each).stem}.png'  # Adjust to match mask filenames\n",
        "        image_np, mask_np = load_image_and_mask(image_path, mask_path)\n",
        "\n",
        "        # Create mask overlay using W&B\n",
        "        mask_img = wb_mask(image_np, mask_np)\n",
        "\n",
        "        # Add image path, original image, and image with mask to the table\n",
        "        table.add_data(str(image_path), wandb.Image(image_np), mask_img)\n",
        "\n",
        "    return table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cALg7cQV4pCV"
      },
      "source": [
        "Logging a single image and a table of images with segmentation masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfH-0cx24ofw"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    entity = WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    group=YOUR_NAME,\n",
        "    name=\"logging_rich_media\",\n",
        "    )\n",
        "\n",
        "# Define paths\n",
        "path_img = Path('images')\n",
        "path_lbl = Path('annotations/trimaps')\n",
        "\n",
        "image_np, mask_np = log_single_images(path_img, path_lbl)\n",
        "\n",
        "# Log single image and segmentation mask\n",
        "run.log({\n",
        "    \"input_image\": wandb.Image(image_np, caption=\"Input Image\"),\n",
        "    \"segmentation_mask\": wb_mask(image_np, mask_np)\n",
        "})\n",
        "\n",
        "#Log tables of images and segmentation mask\n",
        "image_tables = wandb_table_multiple_imags(path_img, path_lbl, 30)\n",
        "\n",
        "# Log the table\n",
        "run.log({\"Segmentation Table\": image_tables})\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awd2IAIa8T17"
      },
      "source": [
        "### Log Sequences of Media\n",
        "\n",
        "If you periodically call `run.log` to log a number (for example, loss), Weights & Biases will automatically render a line plot showing the change in that value over time (a loss curve). You can also log media under a key more than once over the course of an experiment, in which case Weights & Biases will display that media with a step slider so you can scrub over the course of the experiment and see how it changed. This is particularly useful for seeing how model predictions and visualizations of model performance (e.g. a precision/recall curve) change over time. In the example below, we log a `wandb.Image` repeatedly just to demonstrate how this works. Below is an example of doing the same with audio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IFsGTdP8Y6l"
      },
      "outputs": [],
      "source": [
        "%%sh\n",
        "curl https://parade.com/.image/t_share/MTkwNTgwOTUyNjU2Mzg5MjQ1/albert-einstein-quotes-jpg.jpg > image.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Gp6oFiZ8cGC"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageFilter\n",
        "import pandas as pd\n",
        "# Load image with pillow, resize to 512 square\n",
        "im = Image.open(\"./image.jpg\").resize((512, 512))\n",
        "images = []\n",
        "with wandb.init(project = WANDB_PROJECT) as run:\n",
        "\n",
        "  for step in range(10):\n",
        "\n",
        "    # Log image\n",
        "    images.append( (step, wandb.Image(im)))\n",
        "    run.log({\"image\": wandb.Image(im)})\n",
        "\n",
        "    # Apply small Gaussian blur\n",
        "    im = im.filter(ImageFilter.GaussianBlur(radius=1.5))\n",
        "\n",
        "  # Also log the images + associated logging step to a W&B Table\n",
        "  run.log({ \"images_df\": pd.DataFrame( images, columns = [\"step\", \"images\"])})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzrY1MCnB9rT"
      },
      "source": [
        "## Anatomy of a `Run` ü©∫\n",
        "\n",
        "The `Run` stores a detailed record of an experiment within a few specific data structures. The important things to know about are\n",
        "- `run.config` is a dictionary like structure that stores configuration data for a run, like the path to input data or training hyperparameters. You can instatiate the config by passing a dictionary to `wandb.init(config=<config-dict>)`.\n",
        "- `run.history` is a list of dictionaries that stores the sampled historical values of metrics and media over the course of an experiment. We can append a new snapshot of our training metrics by calling `run.log(<metric-dict>)` - full metrics access would be given via `run.scan_history`\n",
        "- `run.summary` is a dictionary for recording summary metrics or media. By default the `summary` will contain the most recent values logged for each metric, you can overwrite and add elements as you like.\n",
        "\n",
        "Use the [Public API](https://docs.wandb.ai/guides/track/public-api-guide#export-data) to interact with the data that you have saved to W&B.\n",
        "\n",
        "To use the Public API, you'll often need the run path which is `<entity>/<project>/<run_id>`, found in the run overview page, or the path to your projects\n",
        "\n",
        "\n",
        "A few basic examples below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Phyqy4LHDpcx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "api = wandb.Api()\n",
        "\n",
        "run_id_for_api = \"\" # fill in the run path you would like to access. This can be found in the run overview page.\n",
        "\n",
        "#Output timestamp and loss saved with run.log({\"accuracy\": loss})\n",
        "run = api.run(f\"{WANDB_ENTITY}/{WANDB_PROJECT}/{run_id_for_api}\")\n",
        "for i, row in run.history().iterrows():\n",
        "    print(row[\"_timestamp\"], row[\"loss\"])\n",
        "\n",
        "#Update metrics for a run, after the run has finished\n",
        "run.summary[\"acc\"] = 0.9\n",
        "run.summary[\"accuracy_histogram\"] = wandb.Histogram(np.random.rand(5))\n",
        "run.summary.update()\n",
        "\n",
        "#Rename a metric in a run, after the run has finished\n",
        "run.summary[\"new_acc\"] = run.summary[\"acc\"]\n",
        "del run.summary[\"acc\"]\n",
        "run.summary.update()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_wn73fEzjqa"
      },
      "source": [
        "#### Querying all points in a run:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TAy-wuSzexE"
      },
      "outputs": [],
      "source": [
        "api = wandb.Api()\n",
        "\n",
        "\n",
        "# access the run object via the api\n",
        "run = api.run('<entity>/<project>/<run_id>') # pass in a run path once again!\n",
        "\n",
        "\n",
        "# full_history is an iterator that yields dictionaries from the run's history, one at a time.\n",
        "full_history = run.scan_history()\n",
        "\n",
        "\n",
        "# list of dictionaries where each dictionary represents a single step's data from the run's history.\n",
        "losses = [row for row in full_history]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dow4ugFKlZ1"
      },
      "source": [
        "# Hyperparameter tuning with sweeps [not available yet]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsyMaAZvMGaY"
      },
      "source": [
        "Fundamentally, a Sweep combines a strategy for trying out a bunch of hyperparameter values with the code that evalutes them. Whether that strategy is as simple as trying every option or as complex as BOHB, Weights & Biases Sweeps have you covered. You just need to define your strategy in the form of a configuration. This is currently not available yet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REWlYGW0INyL"
      },
      "source": [
        "# Artifacts\n",
        "\n",
        "Use W&B Artifacts to track and version data as the inputs and outputs of your W&B Runs. In addition to logging hyperparameters, metadata, and metrics to a run, you can use an artifact to log the dataset used to train the model as input and the resulting model checkpoints as outputs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etKMjkIWz45u"
      },
      "source": [
        "For this demo, we are going to go through the workflow of\n",
        "1. Creating a dataset\n",
        "2. Logging it to wandb\n",
        "3. Processing that dataset\n",
        "4. Logging the processed data to wandb\n",
        "5. Conducting model training\n",
        "6. Viewing the entire lineage of this process in the wandb UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr0H-G_9nDoQ"
      },
      "source": [
        "## Create a Dataset\n",
        "Let's create some datasets that we can work with in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ho7TWxacm8k3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "directory = \"dataset\"\n",
        "os.makedirs(directory, exist_ok=True)\n",
        "file1, file2 = os.path.join(directory, \"file1.csv\"), os.path.join(directory, \"file2.csv\")\n",
        "\n",
        "def generate_dummy_data(num_samples):\n",
        "    data = [\n",
        "        np.random.normal(50, 10, num_samples),\n",
        "        np.random.randint(1, 100, num_samples),\n",
        "        np.random.choice(['A', 'B', 'C', 'D'], num_samples),\n",
        "        np.random.uniform(0.0, 1.0, num_samples)\n",
        "    ]\n",
        "    return zip(*data)\n",
        "\n",
        "def save_to_csv(file, data):\n",
        "    with open(file, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['feature1', 'feature2', 'feature3', 'feature4'])\n",
        "        writer.writerows(data)\n",
        "\n",
        "num_samples = 100\n",
        "save_to_csv(file1, generate_dummy_data(num_samples))\n",
        "save_to_csv(file2, generate_dummy_data(num_samples))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESkuFmftqiX9"
      },
      "source": [
        "The general workflow for creating an Artifact is:\n",
        "\n",
        "1. Intialize a run.\n",
        "2. Create an Artifact.\n",
        "3. Add a any files, directories, or pointers to the new Artifact that you want to track and version.\n",
        "4. Log the artifact in the W&B platform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1750nmI7tw8Y"
      },
      "source": [
        "See the [Artifacts Reference guide](https://www.google.com/url?q=https%3A%2F%2Fdocs.wandb.ai%2Fref%2Fpython%2Fartifact) for more information and other commonly used arguments, including how to store additional metadata.\n",
        "\n",
        "Each time the above `log_artifact` is executed, wandb will create a new version of the Artifact within Weights & Biases if the underlying data has changed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pbfQ-F2yxSB"
      },
      "source": [
        "## Logging this dataset artifact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyX4w4jAqfA2"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT, job_type='log_dataset')\n",
        "\n",
        "artifact = wandb.Artifact(f\"my_first_artifact_{YOUR_NAME}\", type=\"dataset\")\n",
        "# the below will add two individual files to the artifact.\n",
        "artifact.add_file(local_path=f\"{directory}/file1.csv\")\n",
        "artifact.add_file(local_path=f\"{directory}/file2.csv\")\n",
        "\n",
        "# or the below if you wanted to add the entire directory contents.\n",
        "artifact.add_dir(local_path=f\"{directory}\")\n",
        "# explictly log the artifact to Weights & Biases.\n",
        "run.log_artifact(artifact)\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trM9_rvT3Kmp"
      },
      "source": [
        "## Processing and Consuming the dataset Artifact"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0BOwIDM3SQz"
      },
      "source": [
        "When you want to use a specific version of an Artifact in a downstream task, you can specify the specific version you would like to use via either `v0`, `v1`, `v2` and so on, or via specific aliases you may have added. The latest alias always refers to the most recent version of the Artifact logged.\n",
        "\n",
        "The proceeding code snippet specifies that the W&B Run will use an artifact called my_first_artifact with the alias latest. We will take a step to preprocess our dataset and relog it to wandb, so we can see the lineage up until this point:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDr-prUD_7hN"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT, job_type='process_dataset')\n",
        "artifact = run.use_artifact(artifact_or_name=f\"my_first_artifact_{YOUR_NAME}:latest\") # this creates a reference within Weights & Biases that this artifact was used by this run.\n",
        "path = artifact.download() # this downloads the artifact from Weights & Biases to your local system where the code is executing.\n",
        "\n",
        "print(f\"Data directory located at {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzWv3wEfWyjf"
      },
      "outputs": [],
      "source": [
        "processed_directory = \"processed_dataset\"\n",
        "os.makedirs(processed_directory, exist_ok=True)\n",
        "file1, file2 = os.path.join(directory, \"file1_processed.csv\"), os.path.join(directory, \"file2_processed.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY6X_0PuVkSN"
      },
      "outputs": [],
      "source": [
        "# Step 2: Function to process and save the modified CSV data\n",
        "def process(input_file_path, output_file_path):\n",
        "    modified_data = []\n",
        "    with open(input_file_path, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        headers = next(reader)  # Skip headers\n",
        "        for row in reader:\n",
        "            # Example modification: Adjust feature1 by adding a constant\n",
        "            row[0] = str(float(row[0]) + 10)  # Modify feature1\n",
        "            modified_data.append(row)\n",
        "\n",
        "    # Save the modified data to the output path\n",
        "    with open(output_file_path, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(headers)  # Write the headers back\n",
        "        writer.writerows(modified_data)\n",
        "\n",
        "# Apply modification to the CSV files\n",
        "process(os.path.join(path, \"file1.csv\"), os.path.join(processed_directory, \"file1_processed.csv\"))\n",
        "process(os.path.join(path, \"file2.csv\"), os.path.join(processed_directory, \"file2_processed.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYfyaZpTTA0R"
      },
      "outputs": [],
      "source": [
        "# Step 4: Create a new artifact to store the modified data\n",
        "processed_artifact = wandb.Artifact(\n",
        "    f\"my_processed_artifact_{YOUR_NAME}\",\n",
        "    type=\"processed_dataset\"\n",
        ")\n",
        "\n",
        "# Add the modified CSV files to the new artifact\n",
        "processed_artifact.add_file(local_path=f\"{processed_directory}/file1_processed.csv\")\n",
        "processed_artifact.add_file(local_path=f\"{processed_directory}/file1_processed.csv\")\n",
        "\n",
        "# or the below if you wanted to add the entire directory contents.\n",
        "processed_artifact.add_dir(local_path=f\"{processed_directory}\")\n",
        "\n",
        "# Step 5: Log the processed artifact\n",
        "run.log_artifact(processed_artifact)\n",
        "\n",
        "# Finish the run\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnZWCPQoRDiO"
      },
      "source": [
        "## Using our dataset during model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHu4gt7LIbjL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "run = wandb.init(\n",
        "    entity = WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    group=YOUR_NAME,\n",
        "    job_type = \"training\",\n",
        "    config={'param': 42}\n",
        ")\n",
        "\n",
        "# Use our processed dataset in our training run\n",
        "# this creates a reference within Weights & Biases that this artifact was used by this run\n",
        "artifact = run.use_artifact(artifact_or_name=f\"my_processed_artifact_{YOUR_NAME}:latest\")\n",
        "\n",
        "#Save simple neural network model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 50)\n",
        "        self.fc2 = nn.Linear(50, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleModel()\n",
        "\n",
        "# doing some dummy logging here\n",
        "for i in range(5):\n",
        "  run.log({\"acc\": random.random()})\n",
        "\n",
        "# Save the model and log to artifacts\n",
        "model_path = \"simple_model.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "# Log the model as an artifact\n",
        "art = wandb.Artifact(name=f\"simple-model-{YOUR_NAME}\", type=f\"model\")\n",
        "art.add_file(model_path)\n",
        "run.log_artifact(art)\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxhnts5G-ZWe"
      },
      "source": [
        "For more information on ways to customize your Artifact download, including via the command line, see the [Download and Usage guide](https://docs.wandb.ai/guides/artifacts/download-and-use-an-artifact)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv25K2MLztOr"
      },
      "source": [
        "## How can we see which experiments used a particular dataset?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxKTeSO063y-"
      },
      "source": [
        "If we want to see which wandb runs consumed a specific dataset, we can do this two ways:\n",
        "1. Viewing the lineage in the UI\n",
        "2. Programmatically accessing which run names and ids consumed a specific dataset (below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pta-CyiIzw4P"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    entity = WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        ")\n",
        "\n",
        "artifact = run.use_artifact(artifact_or_name=f\"my_processed_artifact_{YOUR_NAME}:latest\")\n",
        "\n",
        "for run in artifact.used_by():\n",
        "  print(run.name)\n",
        "  print(run.summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9egMJyHQsG1j"
      },
      "source": [
        "## Update Artifact version metadata\n",
        "You can update the description, metadata, and alias of an artifact on the W&B platform during or outside a W&B Run.\n",
        "\n",
        "This example changes the description of the my_first_artifact artifact inside a run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqJeGfv8sMcF"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT)\n",
        "artifact = run.use_artifact(artifact_or_name=f\"my_first_artifact_{YOUR_NAME}:latest\")\n",
        "artifact.description = \"This is an edited description.\"\n",
        "artifact.metadata = {\"source\": \"local disk\", \"internal data owner\": \"platform team\"}\n",
        "artifact.save()  # persists changes to an Artifact's properties\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-1UwWmJaGns"
      },
      "source": [
        "## Code Snippet for Logging Reference Artifacts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbeuzUScSseM"
      },
      "source": [
        "Artifacts currently support the following URI schemes:\n",
        "\n",
        "* **http(s)://:** A path to a file accessible over HTTP. The artifact will track checksums in the form of etags and size metadata if the HTTP server supports the ETag and Content-Length response headers.\n",
        "* **s3://:** A path to an object or object prefix in S3. The artifact will track checksums and versioning information (if the bucket has object versioning enabled) for the referenced objects. Object prefixes are expanded to include the objects under the prefix, default up to 100,000 objects.\n",
        "* **gs://:** A path to an object or object prefix in GCS. The artifact will track checksums and versioning information (if the bucket has object versioning enabled) for the referenced objects. Object prefixes are expanded to include the objects under the prefix, default up to 100,000 objects.\n",
        "See below for an example of reference local files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aR6RxpxSr9Y"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                project=WANDB_PROJECT,\n",
        "                job_type=\"upload-references\")\n",
        "artifact = wandb.Artifact(name=f\"local-file-references_{YOUR_NAME}\", type=\"reference-dataset\")\n",
        "artifact.add_reference(\"file:///content/sample_data\", checksum=True)\n",
        "run.log_artifact(artifact)\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozxtC0WVZGB2"
      },
      "source": [
        "## **Artifacts Time-to-live (TTL)**\n",
        "\n",
        "W&B Artifacts supports setting time-to-live policies on each version of an Artifact. The following examples show the use TTL policy in a common Artifact logging workflow. We'll cover:\n",
        "\n",
        "* Setting a TTL policy when creating an Artifact\n",
        "* Retroactively setting TTL for a specific Artifact aliases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoNcRyerZIrY"
      },
      "source": [
        "## Setting TTL on New Artifacts\n",
        "Below we create two new Artifacts from the colab provided sample_data\n",
        "- mnist_test.csv\n",
        "- mnist_train_small.csv\n",
        "\n",
        "Upload them as artifacts files to artifact of type `mnist_dataset` and assign them a TTL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFpn0tBRZK8L"
      },
      "outputs": [],
      "source": [
        "from datetime import timedelta\n",
        "\n",
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                project=WANDB_PROJECT,\n",
        "                job_type=\"raw-data\")\n",
        "\n",
        "raw_mnist_train = wandb.Artifact(\n",
        "    f\"mnist_train_small_{YOUR_NAME}\",\n",
        "    type=\"mnist_dataset\",\n",
        "    description=\"Small MNIST Training Set\"\n",
        ")\n",
        "\n",
        "raw_mnist_train.add_file(\"sample_data/mnist_train_small.csv\")\n",
        "raw_mnist_train.ttl = timedelta(days=10)\n",
        "run.log_artifact(raw_mnist_train, aliases=[\"small\", \"mnist\", \"train\"])\n",
        "\n",
        "raw_mnist_test = wandb.Artifact(\n",
        "    f\"mnist_test_small_{YOUR_NAME}\",\n",
        "    type=\"mnist_dataset\",\n",
        "    description=\"Small MNIST Test Set\"\n",
        ")\n",
        "\n",
        "raw_mnist_test.add_file(\"sample_data/mnist_test.csv\")\n",
        "raw_mnist_test.ttl = timedelta(days=10)\n",
        "run.log_artifact(raw_mnist_test, aliases=[\"small\", \"mnist\", \"test\"])\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ8JNB_nZQmS"
      },
      "source": [
        "# Full pipeline - data upload, model training, linking the best model to registry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCcKODegaSe1"
      },
      "source": [
        "The below pipeline includes:\n",
        "\n",
        "* Data Versioning: The Heart Disease dataset is split into training, validation, and test sets, each logged as a W&B artifact for easy tracking and reproducibility. The training dataset we linked to our dataset registry\n",
        "\n",
        "* Model Training: A neural network is trained on the training set, with performance monitored on the validation set. The best model version is saved and versioned as a W&B artifact which is then linked to our model registry\n",
        "\n",
        "* What It Showcases:\n",
        "How to use W&B for seamless data and model versioning.\n",
        "Best practices for tracking model performance during training and evaluation.\n",
        "Efficient and reproducible ML workflows in a production-ready environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7rMVsil1ZQ4w"
      },
      "outputs": [],
      "source": [
        "COLLECTION_NAME = \"heart-disease\" #@param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "\n",
        "# Load the Heart Disease dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
        "columns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
        "           \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data download and processing\n",
        "raw_data = !CURL https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
        "clean_rows = [\n",
        "    row\n",
        "    for row in raw_data\n",
        "    if row.strip()                  # drop the empty string\n",
        "       and \"--:--:--\" not in row    # progress-bar lines\n",
        "       and \"%\" not in row           # header / percent lines\n",
        "       and \"Dload\" not in row       # second header line\n",
        "]\n",
        "\n",
        "csv = '\\n'.join(clean_rows)\n",
        "data = pd.read_csv(StringIO(csv), header=None, names=columns)\n",
        "\n",
        "# Replace missing values ('?') with NaN and drop rows with NaN values\n",
        "data.replace('?', np.nan, inplace=True)\n",
        "data = data.dropna().astype(float)\n",
        "\n",
        "# Convert target variable: 0 = no heart disease, 1 = presence of heart disease\n",
        "data['target'] = (data['target'] > 0).astype(int)\n",
        "\n",
        "# Shuffle the dataset to ensure random distribution\n",
        "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Perform a train/validation/test split (60/20/20)\n",
        "train_size = int(0.6 * len(data))\n",
        "val_size = int(0.2 * len(data))\n",
        "test_size = len(data) - train_size - val_size\n",
        "\n",
        "train_data = data[:train_size]\n",
        "val_data = data[train_size:train_size + val_size]\n",
        "test_data = data[train_size + val_size:]\n",
        "\n",
        "# Save the entire dataset as a CSV file\n",
        "data.to_csv(\"heart_disease_full_dataset.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TJZOIJRal_k"
      },
      "source": [
        "The following function is used to save a dataset to a file and log it as an artifact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-6y_SYLwampB"
      },
      "outputs": [],
      "source": [
        "# Simple function to save log dataset artifacts\n",
        "def save_and_log_dataset(data, filename, artifact_name, aliases):\n",
        "    # Save the dataset as a CSV file\n",
        "    data.to_csv(filename, index=False)\n",
        "\n",
        "    # Create and log the dataset artifact\n",
        "    dataset_artifact = wandb.Artifact(name=artifact_name, type='dataset')\n",
        "    dataset_artifact.add_file(filename)\n",
        "    wandb.log_artifact(dataset_artifact, aliases=aliases)\n",
        "\n",
        "    return dataset_artifact"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPNJrBpUaxaI"
      },
      "source": [
        "The following cell is going to save our train, validation, and test datasets as artifacts, as well as link our training dataset to our Dataset registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pag6Dbjsayyz"
      },
      "outputs": [],
      "source": [
        "#Upload data to wandb\n",
        "\n",
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                project=WANDB_PROJECT,\n",
        "                group = YOUR_NAME,\n",
        "                job_type=\"heart-disease-data-uploads\",\n",
        "                name = f\"heart_disease_data_uploads_{YOUR_NAME}\",\n",
        "                tags = [\"data-upload\"]\n",
        "                )\n",
        "\n",
        "# Save and log the entire dataset\n",
        "full_artifact = save_and_log_dataset(data, \"heart_disease_full_dataset.csv\", f'heart_disease_full_dataset_{YOUR_NAME}', [\"initial_commit\", \"complete_dataset\"])\n",
        "\n",
        "# Save and log the training dataset\n",
        "train_artifact = save_and_log_dataset(train_data, \"heart_disease_train_dataset.csv\", f'heart_disease_train_dataset_{YOUR_NAME}', [\"initial_commit\", \"train_split\"])\n",
        "\n",
        "# Save and log the validation dataset\n",
        "val_artifact = save_and_log_dataset(val_data, \"heart_disease_val_dataset.csv\", f'heart_disease_validation_dataset_{YOUR_NAME}', [\"initial_commit\", \"validation_split\"])\n",
        "\n",
        "# Save and log the test dataset\n",
        "test_artifact = save_and_log_dataset(test_data, \"heart_disease_test_dataset.csv\", f'heart_disease_test_dataset_{YOUR_NAME}', [\"initial_commit\", \"test_split\"])\n",
        "\n",
        "\n",
        "#Log all dataset to W&B tables for visual analysis\n",
        "run.log({f\"train_data_table_{YOUR_NAME}\": wandb.Table(dataframe=train_data),\n",
        "           f\"test_data_table_{YOUR_NAME}\": wandb.Table(dataframe=test_data),\n",
        "           f\"validation_data_table_{YOUR_NAME}\": wandb.Table(dataframe=val_data)})\n",
        "\n",
        "# Linking Training Dataset to collection in dataset registry\n",
        "target_path = f\"wandb-registry-Dataset/{COLLECTION_NAME}\"\n",
        "\n",
        "# this line will only work if you are logging to a team entity - which has access to Registry\n",
        "run.link_artifact(\n",
        "  artifact=train_artifact,\n",
        "  target_path= target_path\n",
        ")\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR42aD8la41X"
      },
      "source": [
        "The following cell is going to pull the latest training dataset from our training dataset registry so we can start training on it.\n",
        "\n",
        "During training, we save our model checkpoints as artifacts, but we're going to promote our best model from our training to our model registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "wPd14KnQa6eN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbrZUSXZa7rz"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#simple function for loading artifacts from wandb\n",
        "def load_and_split_data(entity, project, artifact_name, your_name, split_name):\n",
        "\n",
        "    artifact_full_name = f'{entity}/{project}/{artifact_name}_{your_name}:latest'\n",
        "    artifact = wandb.use_artifact(artifact_full_name, type='dataset')\n",
        "    artifact_dir = artifact.download()#If you are using local version of artifact, you can simple utilize wandb.use_artifact() only instead of downloading to associated lineage to the artifact\n",
        "\n",
        "    data = pd.read_csv(f\"{artifact_dir}/heart_disease_{split_name}_dataset.csv\")\n",
        "\n",
        "    X = torch.tensor(data.drop(\"target\", axis=1).values, dtype=torch.float32)\n",
        "    y = torch.tensor(data[\"target\"].values, dtype=torch.float32)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# Initialize training run\n",
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                project=WANDB_PROJECT,\n",
        "                group = YOUR_NAME,\n",
        "                job_type=\"heart-disease-training\",\n",
        "                name = f\"heart_disease_training_validation_{YOUR_NAME}\"\n",
        "                )\n",
        "\n",
        "\n",
        "# Explicitly accessing Training data\n",
        "artifact = run.use_artifact('wandb-registry-Dataset/heart-disease:latest', type='dataset')\n",
        "artifact_dir = artifact.download()\n",
        "\n",
        "data = pd.read_csv(f\"{artifact_dir}/heart_disease_train_dataset.csv\")\n",
        "\n",
        "X_train = torch.tensor(data.drop(\"target\", axis=1).values, dtype=torch.float32)\n",
        "y_train = torch.tensor(data[\"target\"].values, dtype=torch.float32)\n",
        "\n",
        "# Load validation data\n",
        "X_val, y_val = load_and_split_data(WANDB_ENTITY, WANDB_PROJECT, 'heart_disease_validation_dataset', YOUR_NAME, 'val')\n",
        "\n",
        "# Define a simple neural network model\n",
        "class HeartDiseaseModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(HeartDiseaseModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.bn3 = nn.BatchNorm1d(32)\n",
        "        self.fc4 = nn.Linear(32, 16)\n",
        "        self.fc5 = nn.Linear(16, 1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = torch.sigmoid(self.fc5(x))\n",
        "        return x\n",
        "\n",
        "model = HeartDiseaseModel(input_size=X_train.shape[1])\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "best_performance = float('inf')\n",
        "version = 1\n",
        "\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train).squeeze()\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate and log training accuracy\n",
        "    predictions = (outputs >= 0.5).float()\n",
        "    train_accuracy = (predictions == y_train).float().mean().item()\n",
        "\n",
        "    run.log({\"train/epoch\": epoch, \"train/train_loss\": loss.item(), \"train/train_accuracy\": train_accuracy})\n",
        "\n",
        "    # Evaluate the model on validation set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val).squeeze()\n",
        "        val_loss = criterion(val_outputs, y_val).item()\n",
        "\n",
        "        # Calculate and log validation accuracy\n",
        "        val_predictions = (val_outputs >= 0.5).float()\n",
        "        val_accuracy = (val_predictions == y_val).float().mean().item()\n",
        "\n",
        "        run.log({\"val/val_loss\": val_loss, \"val/val_accuracy\": val_accuracy})\n",
        "\n",
        "        if val_loss < best_performance:\n",
        "            best_performance = val_loss\n",
        "            model_path = f\"heart_disease_model_v{version}.pth\"\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            artifact = wandb.Artifact(name=f'heart_disease_model_{YOUR_NAME}', type='model')\n",
        "            artifact.add_file(model_path)\n",
        "            wandb.log_artifact(artifact, aliases=[f\"v{version}\", \"best\"])\n",
        "            version += 1\n",
        "\n",
        "# linking the best model to collection in model registry\n",
        "target_path = f\"wandb-registry-Model/{COLLECTION_NAME}\"\n",
        "\n",
        "# Giving it the production alias since this is our best model32\n",
        "# this line will only work if you are logging to a team entity\n",
        "run.link_artifact(\n",
        "  artifact=artifact,\n",
        "  target_path= target_path,\n",
        "  aliases=[\"production\"]\n",
        ")\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svLInvkjIRD9"
      },
      "source": [
        "# Expanded Capabilities\n",
        "\n",
        "Here are some more expanded capabilities of the SDK that we didn't get a chance to touch on today, but am including for reference:\n",
        "1. Custom Charts\n",
        "2. Offline Runs\n",
        "3. Alerts\n",
        "4. Logging Rich Media\n",
        "5. Settings & Environment Variables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb-l3hbPWcco"
      },
      "source": [
        "### Custom Charts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk0HOxb7iHRw"
      },
      "source": [
        "You can create Custom Charts to create charts that are more complex and not offered by the default UI. Log arbitrary tables of data and visualize them exactly how you want. Control details of fonts, colors, and tooltips with the power of Vega. Below is an example of logging a PR curve using a built-in Vega spec.\n",
        "\n",
        "Try even more examples in this [colab](https://tiny.cc/custom-charts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_lhO3BCiG_n"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(project='my-awesome-project')\n",
        "\n",
        "# Generate dummy ground truth and prediction values\n",
        "# Assume we have 3 classes for a multi-class classification problem\n",
        "\n",
        "# Number of samples\n",
        "num_samples = 100\n",
        "\n",
        "# Number of classes\n",
        "num_classes = 3\n",
        "\n",
        "# Ground truth labels (random integers between 0 and num_classes-1)\n",
        "ground_truth = np.random.randint(0, num_classes, size=num_samples)\n",
        "\n",
        "# Prediction probabilities (random float values between 0 and 1, for each class)\n",
        "predictions = np.random.rand(num_samples, num_classes)\n",
        "\n",
        "# Normalize the predictions so that they sum to 1 for each sample (softmax-like behavior)\n",
        "predictions = predictions / predictions.sum(axis=1, keepdims=True)\n",
        "\n",
        "# Optional labels for the classes\n",
        "labels = ['Class 0', 'Class 1', 'Class 2']\n",
        "\n",
        "# Classes to plot (if you want to limit to specific classes)\n",
        "classes_to_plot = [0, 1, 2]\n",
        "\n",
        "# Generate the PR curve using wandb\n",
        "pr_curve_plot = wandb.plot.pr_curve(ground_truth, predictions, labels=labels, classes_to_plot=classes_to_plot)\n",
        "\n",
        "plot = wandb.plot.pr_curve(ground_truth, predictions, labels=None, classes_to_plot=None)\n",
        "\n",
        "run.log({\"pr_curve\": plot})\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei06CM-UXVNq"
      },
      "source": [
        "### Offline Runs üì∂\n",
        "\n",
        "If you're training on an offline machine and want to upload your results to our servers afterwards, we have a feature for you!\n",
        "\n",
        "1. When executing a wandb run, set `mode=\"offline\"` to save the metrics locally, no internet required.\n",
        "\n",
        "2. When you're ready, run wandb init in your directory to set the project name.\n",
        "Run wandb sync `YOUR_RUN_DIRECTORY` to push the metrics to your instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYIA5--4X5Z4"
      },
      "outputs": [],
      "source": [
        "offline_run = wandb.init(\n",
        "    entity = WANDB_ENTITY,\n",
        "    project = WANDB_PROJECT,\n",
        "    group = f\"{YOUR_NAME}\",\n",
        "    name=  f\"exp_run_offline_mode_{YOUR_NAME}\",\n",
        "    mode = \"offline\"\n",
        ")\n",
        "\n",
        "for i in range(10):\n",
        "  wandb.log({\"offline-metric\": i})\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOt8kSJJYOiq"
      },
      "outputs": [],
      "source": [
        "#copy the line below \"You can sync this run to the cloud by running\" to sync your run to your instance\n",
        "#!wandb sync /content/wandb/offline-run-20240816_224835-xs7hs2vy #uncomment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxQjCh2Gl1JF"
      },
      "source": [
        "### üîî Try W&B Alerts\n",
        "\n",
        "**[W&B Alerts](https://docs.wandb.ai/guides/track/alert)** allows you to send alerts, triggered from your Python code, to your Slack or email. There are 2 steps to follow the first time you'd like to send a Slack or email alert, triggered from your code:\n",
        "\n",
        "1) Turn on Alerts in your W&B [User Settings](https://wandb.ai/settings)\n",
        "\n",
        "2) Add `wandb.alert()` to your code:\n",
        "\n",
        "```python\n",
        "wandb.alert(\n",
        "    title=\"Low accuracy\",\n",
        "    text=f\"Accuracy is below the acceptable threshold\"\n",
        ")\n",
        "```\n",
        "\n",
        "See the minimal example below to see how to use `wandb.alert`. You can find the full docs for **[W&B Alerts here](https://docs.wandb.ai/guides/track/alert)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnU9wEmdmHl9"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "run = wandb.init(\n",
        "    entity = WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    group = f\"{YOUR_NAME}\",\n",
        "    name=  f\"exp_run_with_alerts_{YOUR_NAME}\",\n",
        ")\n",
        "\n",
        "#Simulated Code\n",
        "acc_threshold = 0.3\n",
        "for training_step in range(1000):\n",
        "\n",
        "    # Generate a random number for accuracy\n",
        "    accuracy = round(random.random() + random.random(), 3)\n",
        "\n",
        "    # Log accuracy to wandb\n",
        "    wandb.log({\"Accuracy\": accuracy})\n",
        "\n",
        "    # üîî If the accuracy is below the threshold, fire a W&B Alert and stop the run\n",
        "    if accuracy <= acc_threshold:\n",
        "        print(f'Accuracy is: {accuracy}, acceptable threshold is {acc_threshold}')\n",
        "        #Send the wandb Alert\n",
        "        wandb.alert(\n",
        "            title='Low Accuracy',\n",
        "            text=f'Accuracy {accuracy} at step {training_step} is below the acceptable theshold, {acc_threshold}',\n",
        "        )\n",
        "        print('Alert triggered')\n",
        "        break\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQznRZXcLWaW"
      },
      "source": [
        "### Logging Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pATxqm8I40FX"
      },
      "outputs": [],
      "source": [
        "# Initialize a new run\n",
        "run = wandb.init(name=\"audio_waveform\")\n",
        "\n",
        "# Generate audio data\n",
        "fs = 44100 # sampling frequency, Hz\n",
        "length = 3  # length, seconds\n",
        "xs = np.linspace(0, length, num=fs * length)\n",
        "waveform = np.sin(fs * 2 * np.pi / 40  * xs ** 2)\n",
        "\n",
        "# Log audio data\n",
        "run.log({\"examples\":\n",
        "           [wandb.Audio(waveform, caption=\"Boop\", sample_rate=fs)]})\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyYxZTSBsHGM"
      },
      "source": [
        "### Settings and Environment Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKiQgh3FsMJI"
      },
      "source": [
        "### Wandb Run Settings\n",
        "\n",
        "The [Optional environment variables docs](https://docs.wandb.ai/guides/track/environment-variables#optional-environment-variables) outlines a list of environment variables that the user can update to control wandb before the script runs or within the script.\n",
        "\n",
        "## Considerations\n",
        "\n",
        "1. Arguments passed to wandb.init take precedence over the environment. You could call `wandb.init(dir=os.getenv(\"WANDB_DIR\", my_default_override))` if you want to have a default other than the system default when the environment variable isn't set.\n",
        "\n",
        "2. Run settings can be altered directly through `wandb.init()` via settings argument `settings=wandb.Settings(<setting>=<value>)`. For full list of settings, see latest through our github public repo `wandb_settings.py` [file](https://github.com/wandb/wandb/blob/db6129e3a81cd0eaa7f893df2780345fa5878108/wandb/sdk/wandb_settings.py#L296)\n",
        "\n",
        "Some of the frequently asked used envars:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zov5y3BDsShy"
      },
      "outputs": [],
      "source": [
        "# Run Setup\n",
        "os.environ['WANDB_API_KEY'] = \"<your-api-key>\" #Your wandb API key\n",
        "os.environ['WANDB_ENTITY'] = WANDB_ENTITY #Your wandb entity name\n",
        "os.environ['WANDB_PROJECT'] = WANDB_PROJECT #Your wandb project name\n",
        "os.environ['WANDB_DIR'] = \"working_dir\" #Absolute path to store all generated files instead of the wandb directory relative to your training script\n",
        "os.environ['WANDB_CONFIG_PATHS'] = \"config-defaults.yaml\" #Key-value pairs are automatically passed to wandb.config if you create a file called config-defaults.yaml.\n",
        "os.environ['WANDB_CONSOLE'] = \"off\" #Disable stdout / stderr logging.\n",
        "\n",
        "# Artifacts - We will do a deep dive on these during our Artifacts discussion\n",
        "os.environ[\"WANDB_CACHE_DIR\"] = \"art_cache_dir\"\n",
        "os.environ[\"WANDB_DATA_DIR\"] = \"art_staging_dir\"\n",
        "\n",
        "run = wandb.init(\n",
        "    entity = WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    group=YOUR_NAME,\n",
        "    name=\"run_custom_settings\",\n",
        "    #config=wandb.config, #config-defaults.yaml automatically loads into wandb.config\n",
        "    settings = wandb.Settings(run_id=\"ABCDEFGH\", #Override Run ID\n",
        "                              root_dir=\"working_dir\",  #Override working directory\n",
        "                              save_code=False #Disable code saving just for this run\n",
        "                              )\n",
        ")\n",
        "\n",
        "# Insert Training Logic\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQcYytgIHwlq"
      },
      "source": [
        "#Outro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM9Erz8Xm3Ev"
      },
      "source": [
        "Thank you for following with me today as we explored experiment tracking and artifacts in weights and biases. Below you will find additional resources that cover best practices and recommendations.\n",
        "\n",
        "- [Weights & Biases End-to-End Demo Video](https://www.youtube.com/watch?v=tHAFujRhZLA)\n",
        "- [W&B Best Practices Guide](https://wandb.ai/wandb/pytorch-lightning-e2e/reports/W-B-Best-Practices-Guide--VmlldzozNTU1ODY1)\n",
        "- [Weights & Biases Documentation ](https://docs.wandb.ai/)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "QAcJDrCvDCcP",
        "BzrY1MCnB9rT",
        "dbl9f2mjRKCS",
        "Wv25K2MLztOr",
        "9egMJyHQsG1j",
        "OoNcRyerZIrY",
        "svLInvkjIRD9",
        "ei06CM-UXVNq",
        "mxQjCh2Gl1JF"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
