{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch-lightning/Fine_tuning_a_Transformer_with_Pytorch_Lightning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{lightning_hf} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "<!--- @wandbcode{lightning_hf} -->\n",
    "\n",
    "# Train a Model to Check Your Grammar Using W&B, PyTorch Lightning ‚ö°, and ü§ó\n",
    "\n",
    "\n",
    "*Based on Ayush Chaurasia's awesome [W&B report](https://wandb.ai/cayush/bert-finetuning/reports/Sentence-Classification-With-Huggingface-BERT-and-W-B--Vmlldzo4MDMwNA) and [colab](https://colab.research.google.com/drive/1SQ-FOgji8AiyrQ08sIVfDiA8OUw4bC12?usp=sharing) which performs the same task using BERT, vanilla PyTorch, and W&B.*\n",
    "\n",
    "<img src=\"https://wandb.me/mini-diagram\" width=\"650\" alt=\"Weights & Biases\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to train a model to detect ungrammatical sentences from the CoLA dataset. To perform the classification, we will be using Pytorch Lightning ‚ö° to fine tune [DistilBERT](https://arxiv.org/abs/1910.01108), a transformer model from huggingface ü§ó.\n",
    "\n",
    "We'll use Weights & Biases to:\n",
    "- Version our model inputs and outputs using [W&B Artifacts](https://docs.wandb.ai/guides/artifacts), including preprocessing steps, train/validation splits, and model checkpoints\n",
    "- Log and visualize training and validation performance using [W&B's Pytorch Lightning integration](https://docs.wandb.ai/guides/integrations/lightning)\n",
    "- Visualize and explore the raw dataset using [W&B Tables](https://docs.wandb.ai/guides/data-vis)\n",
    "- Orchestrate a hyperparameter search using [W&B Sweeps](https://docs.wandb.ai/guides/sweeps)\n",
    "\n",
    "Be sure to follow the links that each run outputs to your W&B workspace, where you will be able to see...\n",
    "\n",
    "**Your model's performance metrics updating in real time**\n",
    "\n",
    "![](https://i.imgur.com/8yejscO.png)\n",
    "\n",
    "**The raw data as a W&B Table, which you can sort, group, and filter**\n",
    "\n",
    "![](https://imgur.com/oiQ8RE4.png)\n",
    "\n",
    "**An awesome artifact graph showing our full pipeline**\n",
    "\n",
    "![](https://imgur.com/vMJqKw7.png)\n",
    "\n",
    "**Interactive visualizations of how our hyperparameter choices effect model performance**\n",
    "\n",
    "![](https://imgur.com/Twq7V6c.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: torch in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: lightning in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: transformers in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (4.27.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: filelock in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from lightning) (6.0)\n",
      "Requirement already satisfied: fsspec[http]<2025.0,>2021.06.0 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from lightning) (2023.6.0)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from lightning) (0.11.5)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from lightning) (23.2)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from lightning) (1.0.1)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from lightning) (4.65.0)\n",
      "Requirement already satisfied: pytorch-lightning in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from lightning) (2.0.8)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from fsspec[http]<2025.0,>2021.06.0->lightning) (3.11.9)\n",
      "Requirement already satisfied: setuptools in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (59.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from sympy->torch) (1.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (6.0.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.17.1)\n"
     ]
    }
   ],
   "source": [
    "# Install some dependencies\n",
    "!pip install pandas torch lightning transformers\n",
    "!pip install -Uq wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulk import cell\n",
    "import wandb\n",
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightning.pytorch as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1234"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Derandomizing cell\n",
    "pl.seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNote that if you are using W&B local you will need to pass the url of your W&B \\n\\nFor example:\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Note that if you are using W&B local you will need to pass the url of your W&B \n",
    "\n",
    "For example:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muma-wandb\u001b[0m (\u001b[33msmle-demo\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"grammar-checker\"  # W&B project name here\n",
    "entity = None  # your W&B username or teamname here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CoLA Dataset ü•§\n",
    "\n",
    "We‚Äôll fine tune the model on The Corpus of Linguistic Acceptability (CoLA) dataset for single sentence classification. It‚Äôs a set of sentences labeled as grammatically correct or incorrect. It was first published in May of 2018, and is one of the tests included in the ‚ÄúGLUE Benchmark‚Äù on which models like DistilBERT are competing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a [reference artifact](https://docs.wandb.ai/guides/artifacts/references) to store a pointer to the source data. The advantages of doing this are:\n",
    "* Any runs that use this artifact reference will be able to trace their lineage back to the true source\n",
    "* We can use W&B to download the raw data in our code.\n",
    "\n",
    "The cell below starts a run with job type `register-data`. In the context of this run, we:\n",
    " 1. Create an artifact called `cola-raw`\n",
    " 2. Add a reference to the CoLA dataset to our `cola-raw` artifact\n",
    " 3. Log the `cola-raw` artifact to Weights & Biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/umakrishnaswamy/Desktop/smle/c1-onboarding/201/wandb/run-20250804_090212-oe1opddg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smle-demo/grammar-checker/runs/oe1opddg' target=\"_blank\">olive-dawn-1</a></strong> to <a href='https://wandb.ai/smle-demo/grammar-checker' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smle-demo/grammar-checker' target=\"_blank\">https://wandb.ai/smle-demo/grammar-checker</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smle-demo/grammar-checker/runs/oe1opddg' target=\"_blank\">https://wandb.ai/smle-demo/grammar-checker/runs/oe1opddg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">olive-dawn-1</strong> at: <a href='https://wandb.ai/smle-demo/grammar-checker/runs/oe1opddg' target=\"_blank\">https://wandb.ai/smle-demo/grammar-checker/runs/oe1opddg</a><br> View project at: <a href='https://wandb.ai/smle-demo/grammar-checker' target=\"_blank\">https://wandb.ai/smle-demo/grammar-checker</a><br>Synced 6 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250804_090212-oe1opddg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enter the context of a W&B Run object, referenceable with the 'run' variable\n",
    "with wandb.init(entity=entity, project=project, job_type=\"register-data\") as run:\n",
    "\n",
    "  # Construct a wandb.Artifact object\n",
    "  data_source = wandb.Artifact(\"cola-raw\", type=\"dataset\")\n",
    "\n",
    "  # Store a reference to the download URL of the CoLA dataset\n",
    "  data_source.add_reference(\"https://nyu-mll.github.io/CoLA/cola_public_1.1.zip\", name=\"zipfile\")\n",
    "  \n",
    "  # Log the artifact to W&B\n",
    "  run.log_artifact(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization ü™ô\n",
    "\n",
    "The cell below defines the function `tokenize_data`, which transforms a list of sentences and a list of labels into a tuple of `torch.tensor` objects which can be consumed by the transormer model we'll be using. The 3 tensors returned are the tokenized form of the sentences, the attention masks indicating which tokens in each sentence correspond to actual words, and a tensor containing the original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(sentences, labels):\n",
    "\n",
    "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "\n",
    "  # Get BertTokenizer from transformers\n",
    "  tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "  # For every sentence...\n",
    "  for sent in sentences:\n",
    "    \n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                      sent,                      # Sentence to encode.\n",
    "                      add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                      max_length = 64,           # Pad & truncate all sentences.\n",
    "                      padding='max_length',\n",
    "                      return_attention_mask = True,   # Construct attn. masks.\n",
    "                      return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "  # Convert the lists into tensors.\n",
    "  input_ids = torch.cat(input_ids, dim=0)\n",
    "  attention_masks = torch.cat(attention_masks, dim=0)\n",
    "  labels = torch.tensor(labels)\n",
    "  return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below executes a run of type `preprocess-data`, which will\n",
    "1. Download the CoLA dataset using the reference artifact we logged previously\n",
    "2. Log the entire dataset to W&B as a Table\n",
    "3. Use the function `tokenize_data` to transform each sentence into a sequence of tokens and an attention mask\n",
    "4. Log the preprocessed data as an artifact to W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/umakrishnaswamy/Desktop/smle/c1-onboarding/201/wandb/run-20250804_090215-a0rm08ib</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smle-demo/grammar-checker/runs/a0rm08ib' target=\"_blank\">celestial-monkey-2</a></strong> to <a href='https://wandb.ai/smle-demo/grammar-checker' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smle-demo/grammar-checker' target=\"_blank\">https://wandb.ai/smle-demo/grammar-checker</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smle-demo/grammar-checker/runs/a0rm08ib' target=\"_blank\">https://wandb.ai/smle-demo/grammar-checker/runs/a0rm08ib</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /Users/umakrishnaswamy/Desktop/smle/c1-onboarding/201/artifacts/cola-raw:v0/zipfile\n",
      "   creating: /Users/umakrishnaswamy/Desktop/smle/c1-onboarding/201/cola_public\n",
      "  inflating: cola_public/README      \n",
      "   creating: /Users/umakrishnaswamy/Desktop/smle/c1-onboarding/201/cola_public/tokenized\n",
      "  inflating: cola_public/tokenized/in_domain_dev.tsv  \n",
      "  inflating: cola_public/tokenized/in_domain_train.tsv  \n",
      "  inflating: cola_public/tokenized/out_of_domain_dev.tsv  \n",
      "   creating: /Users/umakrishnaswamy/Desktop/smle/c1-onboarding/201/cola_public/raw\n",
      "  inflating: cola_public/raw/in_domain_dev.tsv  \n",
      "  inflating: cola_public/raw/in_domain_train.tsv  \n",
      "  inflating: cola_public/raw/out_of_domain_dev.tsv  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f58183355649e6bfd5b769d1424597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">celestial-monkey-2</strong> at: <a href='https://wandb.ai/smle-demo/grammar-checker/runs/a0rm08ib' target=\"_blank\">https://wandb.ai/smle-demo/grammar-checker/runs/a0rm08ib</a><br> View project at: <a href='https://wandb.ai/smle-demo/grammar-checker' target=\"_blank\">https://wandb.ai/smle-demo/grammar-checker</a><br>Synced 6 W&B file(s), 1 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250804_090215-a0rm08ib/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(entity=entity, project=project, job_type=\"preprocess-data\") as run:\n",
    "  \n",
    "  # Download the raw cola data from the 'zipfile' reference we added to the cola-raw artifact.\n",
    "  raw_data_artifact = run.use_artifact(\"cola-raw:latest\")\n",
    "  zip_path = raw_data_artifact.get_entry(\"zipfile\").download()\n",
    "  !unzip -o $zip_path  # jupyter hack to unzip data :P\n",
    "  \n",
    "  # Read in the raw data, log it to W&B as a wandb.Table\n",
    "  df = pd.read_csv(\n",
    "    \"./cola_public/raw/in_domain_train.tsv\", \n",
    "    delimiter='\\t', \n",
    "    header=None, \n",
    "    names=['sentence_source', 'label', 'label_notes', 'sentence']\n",
    "  )\n",
    "  run.log({\"raw-data\": wandb.Table(dataframe=df)})\n",
    "  \n",
    "  # Perform tokenization and store as a TensorDataset\n",
    "  input_ids, attention_masks, labels = tokenize_data(df.sentence.values, df.label.values)\n",
    "  preprocessed_data = torch.utils.data.TensorDataset(input_ids, attention_masks, labels)\n",
    "  \n",
    "  # 1. Create an artifact called preprocessed-data\n",
    "  # 2. Save the dataset to a local fil called preprocessed-data.pt\n",
    "  # 3. Add that file to the preprocessed-data artifact\n",
    "  # 4. Log the artifact to W&B\n",
    "  data_artifact = wandb.Artifact(\"preprocessed-data\", type=\"dataset\")\n",
    "  with open(\"preprocessed-data.pt\", \"wb\") as f:\n",
    "    torch.save(preprocessed_data, f)\n",
    "  data_artifact.add_file(\"preprocessed-data.pt\", name=\"dataset\")\n",
    "  run.log_artifact(data_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Our Data ü™ì\n",
    "\n",
    "For our training process, we want to split the data into a train and validation set. The train set is the data we will use to update the model parameters, while the validation set will be a smaller segment of data that we use to test whether our model is generalizing to examples that it hasn't been trained on.\n",
    "\n",
    "The cell below executes a `wandb.Run` with `job_type=\"split-data\"`. In the context of this run we will:\n",
    "\n",
    "1. Download the `preprocessed-data` artifact logged by our previous run\n",
    "2. Use the `random_split` function from `torch` to perform a randomn 90/10 test/valiation split on the preprocessed data\n",
    "3. Store the split datasets in a new artifact called `split-dataset`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/umakrishnaswamy/Desktop/smle/c1-onboarding/201/wandb/run-20250804_090523-rfhuj5kf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smle-demo/grammar-checker/runs/rfhuj5kf' target=\"_blank\">chocolate-valley-3</a></strong> to <a href='https://wandb.ai/smle-demo/grammar-checker' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smle-demo/grammar-checker' target=\"_blank\">https://wandb.ai/smle-demo/grammar-checker</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smle-demo/grammar-checker/runs/rfhuj5kf' target=\"_blank\">https://wandb.ai/smle-demo/grammar-checker/runs/rfhuj5kf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">chocolate-valley-3</strong> at: <a href='https://wandb.ai/smle-demo/grammar-checker/runs/rfhuj5kf' target=\"_blank\">https://wandb.ai/smle-demo/grammar-checker/runs/rfhuj5kf</a><br> View project at: <a href='https://wandb.ai/smle-demo/grammar-checker' target=\"_blank\">https://wandb.ai/smle-demo/grammar-checker</a><br>Synced 6 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250804_090523-rfhuj5kf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(entity=entity, project=project, job_type=\"split-data\") as run:\n",
    "\n",
    "  # Download the preprocessed data\n",
    "  pp_data_artifact = run.use_artifact(\"preprocessed-data:latest\")\n",
    "  data_path = pp_data_artifact.get_entry(\"dataset\").download()\n",
    "  dataset = torch.load(data_path)\n",
    "\n",
    "  # Calculate the number of samples to include in each set.\n",
    "  train_size = int(0.9 * len(dataset))\n",
    "  val_size = len(dataset) - train_size\n",
    "\n",
    "  # Divide the dataset by randomly selecting samples.\n",
    "  train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "  # Construct a new artifact\n",
    "  split_data_artifact = wandb.Artifact(\"split-dataset\", type=\"dataset\")\n",
    "  \n",
    "  # Save the dataset splits to disk\n",
    "  torch.save(train_dataset, \"train.pt\")\n",
    "  torch.save(val_dataset, \"validation.pt\")\n",
    "  \n",
    "  # Add the data splits to the artifact\n",
    "  split_data_artifact.add_file(\"train.pt\", name=\"train-data\")\n",
    "  split_data_artifact.add_file(\"validation.pt\", name=\"validation-data\")\n",
    "  \n",
    "  # Log the artifact to W&B\n",
    "  run.log_artifact(split_data_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Our Model ‚ö°\n",
    "\n",
    "We define our model and the associated training + validation procedures in the `LightningModule` below. The model itself is a pre-trained `DistilBertForSequenceClassification` with two labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifier(pl.LightningModule):\n",
    "  \n",
    "  def __init__(self, learning_rate=5e-5):\n",
    "    super(SentenceClassifier, self).__init__()\n",
    "    \n",
    "    # Load pretrained distilbert-base-uncased configured for classification with 2 labels\n",
    "    self.model = transformers.DistilBertForSequenceClassification.from_pretrained(\n",
    "      \"distilbert-base-uncased\", \n",
    "      num_labels = 2, \n",
    "      output_attentions = False, # Whether the model returns attentions weights.\n",
    "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "    )\n",
    "    self.learning_rate = learning_rate\n",
    "\n",
    "  def training_step(self, batch, batch_no):\n",
    "    \"\"\"\n",
    "    This function overrides the pl.LightningModule class. \n",
    "    \n",
    "    When trainer.fit is called, each batch from the provided data loader is fed \n",
    "    to this function successively. \n",
    "    \"\"\"\n",
    "    ids, masks, labels = batch\n",
    "    outputs = self.model(ids, attention_mask=masks, labels=labels)\n",
    "    preds = torch.argmax(outputs[\"logits\"], axis=1)\n",
    "    correct = sum(preds.flatten() == labels.flatten())\n",
    "    self.log(\"train/loss\", outputs[\"loss\"], on_step=True, on_epoch=True)\n",
    "    self.log(\"train/acc\", correct/len(ids), on_step=True, on_epoch=True)\n",
    "    return outputs[\"loss\"]\n",
    "\n",
    "  def validation_step(self, batch, batch_no):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    ids, masks, labels = batch\n",
    "    outputs = self.model(ids, attention_mask=masks, labels=labels)\n",
    "    preds = torch.argmax(outputs[\"logits\"], axis=1)\n",
    "    correct = sum(preds.flatten() == labels.flatten())\n",
    "    self.log(\"val/loss\", outputs[\"loss\"], on_step=False, on_epoch=True)\n",
    "    self.log(\"val/acc\", correct/len(ids), on_step=False, on_epoch=True)\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    \"\"\"\n",
    "    This is overriding a LightningModule method that is called to return the\n",
    "    optimizer used for training.\n",
    "    \"\"\"\n",
    "    return transformers.AdamW(\n",
    "        self.model.parameters(),\n",
    "        lr = self.learning_rate, \n",
    "        eps = 1e-8 \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Tracking Our Model üìâ\n",
    "\n",
    "In the cell below, we define a function `train` which sets up and performs training in the context of a W&B run. The train function takes a configuration dictionary as input then passes it to `wandb.init` via the `config` keyword argument. We use the values saved in the `wandb.config` object associated with the run to set the parameters of our trainer and data loaders. This is a crucial best practice to ensure that the values logged in the `config` object (and displayed in the run table of the W&B app) represent the actual parameters of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config={\"learning_rate\": 5e-5, \"batch_size\": 16, \"epochs\": 2}):\n",
    "  \n",
    "  with wandb.init(project=project, entity=entity, job_type=\"train\", config=config) as run:  \n",
    "\n",
    "    # Load the datasets from the split-dataset artifact\n",
    "    data = run.use_artifact(\"split-dataset:latest\")\n",
    "    train_dataset = torch.load(data.get_entry(\"train-data\").download())\n",
    "    val_dataset = torch.load(data.get_entry(\"validation-data\").download())\n",
    "\n",
    "    # Extract the config object associated with the run\n",
    "    config = run.config\n",
    "    \n",
    "    # Construct our LightningModule with the learning rate from the config object\n",
    "    model = SentenceClassifier(learning_rate=config.learning_rate)\n",
    "\n",
    "    # This logger is used when we call self.log inside the LightningModule\n",
    "    logger = pl.loggers.WandbLogger(experiment=run, log_model=True)\n",
    "    \n",
    "    # Use as many GPUs as are available\n",
    "    gpus = -1 if torch.cuda.is_available() else 0\n",
    "    \n",
    "    # Construct a Trainer object with the W&B logger we created and epoch set by the config object\n",
    "    trainer = pl.Trainer(max_epochs=config.epochs, logger=logger)\n",
    "    \n",
    "    # Build data loaders for our datasets, using the batch_size from our config object\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size)\n",
    "    val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=config.batch_size)\n",
    "    \n",
    "    # Execute training\n",
    "    trainer.fit(model, train_data_loader, val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/umakrishnaswamy/Desktop/smle/c1-onboarding/201/wandb/run-20250804_090530-omzorw8i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smle-demo/grammar-checker/runs/omzorw8i' target=\"_blank\">vocal-eon-4</a></strong> to <a href='https://wandb.ai/smle-demo/grammar-checker' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smle-demo/grammar-checker' target=\"_blank\">https://wandb.ai/smle-demo/grammar-checker</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smle-demo/grammar-checker/runs/omzorw8i' target=\"_blank\">https://wandb.ai/smle-demo/grammar-checker/runs/omzorw8i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3dc87040a1542538d0e5cc8e983f99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e2d854ea6a4dbe8eec7d7d8fc6665d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                                | Params\n",
      "--------------------------------------------------------------\n",
      "0 | model | DistilBertForSequenceClassification | 67.0 M\n",
      "--------------------------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.0 M    Total params\n",
      "267.820   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de13c6566d624957ab4bc1d7bf46a2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "[Uma-Krishnaswamy-FF7FM3XLQL:84767] *** Process received signal ***\n",
      "[Uma-Krishnaswamy-FF7FM3XLQL:84767] Signal: Bus error: 10 (10)\n",
      "[Uma-Krishnaswamy-FF7FM3XLQL:84767] Signal code: Invalid address alignment (1)\n",
      "[Uma-Krishnaswamy-FF7FM3XLQL:84767] Failing at address: 0x16b794000\n",
      "[Uma-Krishnaswamy-FF7FM3XLQL:84767] [ 0] 0   libsystem_platform.dylib            0x0000000194428624 _sigtramp + 56\n",
      "[Uma-Krishnaswamy-FF7FM3XLQL:84767] [ 1] 0   MetalPerformanceShadersGraph        0x00000001f8640bac _ZN4llvm15SmallVectorImplIyE6insertEPymy + 140\n",
      "[Uma-Krishnaswamy-FF7FM3XLQL:84767] [ 2] 0   MetalPerformanceShadersGraph        0x00000001f8ca5a34 _ZNK4mlir12_GLOBAL__N_113ConvertGather31matchAndRewriteWithStaticShapesENS_3mps8GatherOpENS2_15GatherOpAdaptorERNS_25ConversionPatternRewriterE + 1176\n",
      "[Uma-Krishnaswamy-FF7FM3XLQL:84767] [ 3] [Uma-Krishnaswamy-FF7FM3XLQL:84767] *** Process received signal ***\n",
      "[Uma-Krishnaswamy-FF7FM3XLQL:84767] Signal: Segmentation fault: 11 (11)\n",
      "[Uma-Krishnaswamy-FF7FM3XLQL:84767] Signal code: Invalid permissions (2)\n",
      "[Uma-Krishnaswamy-FF7FM3XLQL:84767] Failing at address: 0x1\n",
      "[Uma-Krishnaswamy-FF7FM3XLQL:84767] [ 0] 0   libsystem_platform.dylib            0x0000000194428624 _sigtramp + 56\n",
      "[Uma-Krishnaswamy-FF7FM3XLQL:84767] [ 1] 0   dyld                                0x00000001940826e8 _ZN5dyld44APIs17findImageMappedAtEPKvPPKN5dyld311MachOLoadedEPbPPKcPS2_PyPhPPKNS_6LoaderE + 720\n",
      "[Uma-Krishnaswamy-FF7FM3XLQL:84767] [ 2] 0   dyld                                0x00000001940826e8 _ZN5dyld44APIs17findImageMappedAtEPKvPPKN5dyld311MachOLoadedEPbPPKcPS2_PyPhPPKNS_6LoaderE + 720\n",
      "[Uma-Krishnaswamy-FF7FM3XLQL:84767] [ 3] 0   dyld                                0x0000000194082db0 _ZN5dyld44APIs6dladdrEPKvP7dl_info + 192\n",
      "[Uma-Krishnaswamy-FF7FM3XLQL:84767] [ 4] 0   libsystem_c.dylib                   0x00000001942b5964 backtrace_symbols_fd + 112\n",
      "[Uma-Krishnaswamy-FF7FM3XLQL:84767] [ 5] "
     ]
    }
   ],
   "source": [
    "train()  # Run training with default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a Hyperparameter Sweep üßπ\n",
    "\n",
    "W&B sweeps allow you to optimize your model hyperparameters with minimal effort. In general, the workflow of sweeps is:\n",
    "1. Construct a dictionary or YAML file that defines the hyperparameter space \n",
    "2. Call `wandb.sweep(<sweep-dict>)` from the python library or `wandb sweep <yaml-file>` from the command line to initialize the sweep in W&B\n",
    "3. Run `wandb.agent(<sweep-id>)` (python lib) or `wandb agent <sweep-id>` (cli) to start a sweep agent to continuously:\n",
    "  - pull hyperparameter combinations from W&B\n",
    "  - run training with the given hyperparameters \n",
    "  - log training metrics back to W&B\n",
    "\n",
    "<img src=\"https://i.imgur.com/zlbw3vQ.png\" alt=\"sweeps-diagram\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the sweeps workflow laid out above by:\n",
    "1. Creating a `sweep_config` dictionary describing our hyperparameter space and objective\n",
    "  - The hyperparameters we will sweep over are `learning_rate`, `batch_size`, and `epochs`\n",
    "  - Our objective in this sweep is to maximize the `validation/epoch_acc` metric logged to W&B\n",
    "  - We will use the `random` strategy, which means we will sample uniformly from the parameter space indefinitely\n",
    "2. Calling `wandb.sweep(sweep_config)` to create the sweep in our W&B project\n",
    "  - `wandb.sweep` will return a unique id for the sweep, saved as `sweep_id`\n",
    "3. Calling `wandb.agent(sweep_id, function=train)` to create an agent that will execute training with different hyperparameter combinations\n",
    "  - The agent will repeatedly query W&B for hyperparameter combinations\n",
    "  - When `wandb.init` is called within an agent, the `config` dictionary of the returned `run` will be populated with the next hyperparameter combination in the sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',  # Randomly sample the hyperparameter space (alternatives: grid, bayes)\n",
    "    'metric': {  # This is the metric we are interested in maximizing\n",
    "      'name': 'validation/epoch_acc',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    # Paramters and parameter values we are sweeping across\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [5e-5, 3e-5, 2e-5]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32]\n",
    "        },\n",
    "        'epochs':{\n",
    "            'values': [1, 2]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: b2e23eq4\n",
      "Sweep URL: https://wandb.ai/smle-demo/grammar-checker/sweeps/b2e23eq4\n"
     ]
    }
   ],
   "source": [
    "# Create the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=project, entity=entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mohq4cca with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muma-wandb\u001b[0m (\u001b[33msmle-demo\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'grammar-checker' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/umakrishnaswamy/Desktop/smle/c1-onboarding/201/wandb/run-20250804_092556-mohq4cca</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smle-demo/grammar-checker/runs/mohq4cca' target=\"_blank\">daily-sweep-1</a></strong> to <a href='https://wandb.ai/smle-demo/grammar-checker' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/smle-demo/grammar-checker/sweeps/b2e23eq4' target=\"_blank\">https://wandb.ai/smle-demo/grammar-checker/sweeps/b2e23eq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smle-demo/grammar-checker' target=\"_blank\">https://wandb.ai/smle-demo/grammar-checker</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/smle-demo/grammar-checker/sweeps/b2e23eq4' target=\"_blank\">https://wandb.ai/smle-demo/grammar-checker/sweeps/b2e23eq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smle-demo/grammar-checker/runs/mohq4cca' target=\"_blank\">https://wandb.ai/smle-demo/grammar-checker/runs/mohq4cca</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                                | Params\n",
      "--------------------------------------------------------------\n",
      "0 | model | DistilBertForSequenceClassification | 67.0 M\n",
      "--------------------------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.0 M    Total params\n",
      "267.820   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f643cc79a940efa1de99f4395f553a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umakrishnaswamy/anaconda3/envs/tf/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    }
   ],
   "source": [
    "# Run an agent üïµÔ∏è to try out 5 hyperparameter combinations\n",
    "wandb.agent(sweep_id, function=train, count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
